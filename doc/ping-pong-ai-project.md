---
marp: true
theme: default
paginate: true
style: |
  section {
    font-family: 'Segoe UI', sans-serif;
    background-color: #f5f5f5;
  }
  h1 { color: #0066cc; }
  h2 { color: #0088dd; }
  table { font-size: 0.85em; }
  pre { background-color: #f0f0f0; padding: 10px; border-radius: 5px; }
---

# æ©Ÿå™¨å­¸ç¿’å°ˆé¡Œ
## ä¹’ä¹“çƒéŠæˆ² AI ç³»çµ±

### å°ˆé¡Œç°¡å ±

---

# ç›®éŒ„

1. éœ€æ±‚åˆ†æ
2. ç³»çµ±åˆ†æ
3. ç³»çµ±è¨­è¨ˆ
4. ç·¨ç¢¼å¯¦ç¾
5. é©—è­‰èˆ‡æ¸¬è©¦

---

# ç¬¬ä¸€éƒ¨åˆ†ï¼šéœ€æ±‚åˆ†æ

---

## éœ€æ±‚ï¼šåŠŸèƒ½æ€§éœ€æ±‚

### æ ¸å¿ƒåŠŸèƒ½

| åŠŸèƒ½ID | åŠŸèƒ½æè¿° | å„ªå…ˆç´š |
|--------|--------|--------|
| F1 | éŠæˆ²ç’°å¢ƒå»ºç«‹èˆ‡åˆå§‹åŒ– | å¿…è¦ |
| F2 | çƒç‰©ç†é‹å‹•æ¨¡æ“¬ | å¿…è¦ |
| F3 | çƒæ‹æ§åˆ¶èˆ‡ç§»å‹• | å¿…è¦ |
| F4 | AI æ±ºç­–èˆ‡å‹•ä½œåŸ·è¡Œ | å¿…è¦ |
| F5 | éŠæˆ²ç‹€æ…‹è·Ÿè¹¤èˆ‡å¾—åˆ†è¨ˆç®— | å¿…è¦ |
| F6 | è¨“ç·´æ¨¡å¼èˆ‡é æ¸¬æ¨¡å¼åˆ‡æ› | é‡è¦ |
| F7 | æ¨¡å‹ä¿å­˜èˆ‡åŠ è¼‰ | é‡è¦ |
| F8 | è¦–è¦ºåŒ–çµæœå‘ˆç¾ | è¼”åŠ© |

---

## éœ€æ±‚ï¼šè¦æ ¼éœ€æ±‚

### æ€§èƒ½è¦æ ¼

| é …ç›® | è¦æ ¼ | èªªæ˜ |
|------|------|------|
| éŠæˆ²å¹€ç‡ | 60 FPS | ç¢ºä¿æµæš¢çš„éŠæˆ²é«”é©— |
| çƒé€Ÿåº¦ç¯„åœ | 5~15 px/frame | é›£åº¦èª¿æ•´åƒæ•¸ |
| çƒæ‹åæ‡‰æ™‚é–“ | â‰¤50 ms | AI æ±ºç­–å»¶é² |
| è¨“ç·´æ”¶æ–‚æ™‚é–“ | â‰¤2 å°æ™‚ | 1,000 episodes |
| è¨˜æ†¶é«”éœ€æ±‚ | â‰¤2 GB | æ¨¡å‹èˆ‡ç·©è¡å€ |

---

## éœ€æ±‚ï¼šæ€§èƒ½éœ€æ±‚

### æº–ç¢ºåº¦èˆ‡ç©©å®šæ€§

| æŒ‡æ¨™ | ç›®æ¨™å€¼ | æ¸¬è©¦æ–¹æ³• |
|------|--------|---------|
| æ¥çƒæˆåŠŸç‡ | â‰¥85% | æ¸¬è©¦é›†è©•ä¼° |
| å¹³å‡éŠæˆ²æ™‚é•· | â‰¥100 çƒ | é€£çºŒé‹è¡Œæ¸¬è©¦ |
| æ¨¡å‹ç©©å®šæ€§ | æ¨™æº–å·®<10% | å¤šæ¬¡è¨“ç·´å°æ¯” |
| æ¨ç†å»¶é² | <30 ms/action | æ€§èƒ½åˆ†æå·¥å…· |

---

## éœ€æ±‚ï¼šé©—æ”¶æ–¹æ³•

### é©—æ”¶æ¸¬è©¦æ–¹æ¡ˆ

1. **åŠŸèƒ½é©—æ”¶**
   - æ¯å€‹åŠŸèƒ½å–®ç¨æ¸¬è©¦ (é»‘ç®±æ¸¬è©¦)
   - åŠŸèƒ½äº¤äº’æ¸¬è©¦ (é›†æˆæ¸¬è©¦)

2. **æ€§èƒ½é©—æ”¶**
   - æ€§èƒ½åŸºæº–æ¸¬è©¦ (benchmark)
   - å£“åŠ›æ¸¬è©¦ (100+ é€£çºŒéŠæˆ²)

3. **æ¨¡å‹é©—æ”¶**
   - æº–ç¢ºåº¦è©•ä¼° (æ¸¬è©¦é›†)
   - æ³›åŒ–èƒ½åŠ›æ¸¬è©¦ (ä¸åŒåˆå§‹æ¢ä»¶)

4. **ç”¨æˆ¶é©—æ”¶**
   - è¦–è¦ºåŒ–çµæœè©•ä¼°
   - æ–‡ä»¶å®Œæ•´æ€§æª¢æŸ¥

---

# ç¬¬äºŒéƒ¨åˆ†ï¼šç³»çµ±åˆ†æ

---

## åˆ†æï¼šç³»çµ±é‚Šç•Œèˆ‡åˆ©å®³é—œä¿‚äºº

### ç³»çµ±é‚Šç•Œ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     ä¹’ä¹“çƒéŠæˆ²ç³»çµ±              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ éŠæˆ²å¼•æ“ (ç‰©ç†æ¨¡æ“¬)            â”‚
â”‚ â€¢ AI æ±ºç­–æ¨¡çµ„ (ML æ¨¡å‹)         â”‚
â”‚ â€¢ è¨“ç·´æ¨¡çµ„ (RL Framework)       â”‚
â”‚ â€¢ æ•¸æ“šè™•ç†æ¨¡çµ„                  â”‚
â”‚ â€¢ UI/è¦–è¦ºåŒ–                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†• (è¼¸å…¥/è¼¸å‡º)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   å¤–éƒ¨ç³»çµ±èˆ‡ç’°å¢ƒ                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ æ“ä½œç³»çµ± (Windows/Linux)      â”‚
â”‚ â€¢ Python Runtime                â”‚
â”‚ â€¢ GPU/CPU                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## åˆ†æï¼šç”¨ä¾‹åœ– (Use Case Diagram)

| ç”¨ä¾‹ | åƒèˆ‡è€… | æè¿° |
|------|--------|------|
| è¨“ç·´ AI æ¨¡å‹ | é–‹ç™¼è€… | ä½¿ç”¨è¨“ç·´æ•¸æ“šé€²è¡Œå¼·åŒ–å­¸ç¿’ |
| åŸ·è¡ŒéŠæˆ² | ç©å®¶/ç³»çµ± | å•Ÿå‹•éŠæˆ²ä¸¦é€²è¡Œå°å±€ |
| è©•ä¼°æ¨¡å‹ | é–‹ç™¼è€… | åœ¨æ¸¬è©¦é›†ä¸Šé€²è¡Œæ€§èƒ½è©•ä¼° |
| ä¿å­˜/åŠ è¼‰æ¨¡å‹ | ç³»çµ± | æŒä¹…åŒ–å­˜å„²å’Œæ¢å¾©æ¨¡å‹ |
| ç›£æ§è¨“ç·´é€²åº¦ | é–‹ç™¼è€… | å¯¦æ™‚æŸ¥çœ‹æå¤±å‡½æ•¸å’Œçå‹µ |

---

## åˆ†æï¼šè³‡æ–™æµåœ– (DFD)

### Level 0 - ä¸Šä¸‹æ–‡åœ–

```
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   ç©å®¶æŒ‡ä»¤  â”‚
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
          â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  ä¹’ä¹“çƒéŠæˆ²ç³»çµ±   â”‚
   â”‚  (AI æ±ºç­–å¼•æ“)   â”‚
   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   éŠæˆ²è¼¸å‡º  â”‚
   â”‚  (è¦–è¦ºåŒ–)   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## åˆ†æï¼šé—œéµæ•¸æ“šæµ

| æ•¸æ“šæµ | ä¾†æº | ç›®æ¨™ | å…§å®¹ |
|--------|------|------|------|
| D1 | éŠæˆ²å¼•æ“ | AI æ¨¡çµ„ | éŠæˆ²ç‹€æ…‹ (çƒä½ã€é€Ÿåº¦ã€æ‹ä½) |
| D2 | AI æ¨¡çµ„ | éŠæˆ²å¼•æ“ | æ±ºç­–å‹•ä½œ (å·¦ç§»/å³ç§»/ä¸å‹•) |
| D3 | éŠæˆ²å¼•æ“ | è¨“ç·´æ¨¡çµ„ | è¨“ç·´æ•¸æ“š (ç‹€æ…‹ã€å‹•ä½œã€çå‹µ) |
| D4 | è¨“ç·´æ¨¡çµ„ | å­˜å„² | æ¨¡å‹æ¬Šé‡ã€è¨“ç·´æ—¥èªŒ |

---

# ç¬¬ä¸‰éƒ¨åˆ†ï¼šç³»çµ±è¨­è¨ˆ

---

## è¨­è¨ˆï¼šç³»çµ±æ¨¡çµ„åˆ†æ”¯åœ–

```mermaid
graph TB
    A[ä¹’ä¹“çƒéŠæˆ²ç³»çµ±] --> B[éŠæˆ²å¼•æ“æ¨¡çµ„]
    A --> C[AI æ±ºç­–æ¨¡çµ„]
    A --> D[è¨“ç·´æ¨¡çµ„]
    A --> E[æ•¸æ“šç®¡ç†æ¨¡çµ„]
    A --> F[è¦–è¦ºåŒ–æ¨¡çµ„]
    
    B --> B1[ç‰©ç†å¼•æ“]
    B --> B2[ç‹€æ…‹ç®¡ç†å™¨]
    B --> B3[äº‹ä»¶è™•ç†å™¨]
    
    C --> C1[DQN æ¨¡å‹]
    C --> C2[ç‰¹å¾µæå–å™¨]
    C --> C3[å‹•ä½œé¸æ“‡å™¨]
    
    D --> D1[RL è¨“ç·´è¿´åœˆ]
    D --> D2[ç¶“é©—é‡æ”¾ç·©è¡]
    D --> D3[çå‹µè¨ˆç®—]
    
    E --> E1[æ•¸æ“šåŠ è¼‰å™¨]
    E --> E2[æ¨¡å‹æŒä¹…åŒ–]
    
    F --> F1[å¯¦æ™‚éŠæˆ²æ¸²æŸ“]
    F --> F2[è¨“ç·´ç›£æ§é¢æ¿]
    F --> F3[è©•ä¼°çµæœå±•ç¤º]
```

---

## è¨­è¨ˆï¼šMSC åœ– - è¨“ç·´æµç¨‹

```mermaid
sequenceDiagram
    participant Player as ç©å®¶/è¨“ç·´å™¨
    participant GameEngine as éŠæˆ²å¼•æ“
    participant AI as AI æ±ºç­–æ¨¡çµ„
    participant RLTrainer as RL è¨“ç·´æ¨¡çµ„
    participant Model as ç¥ç¶“ç¶²çµ¡æ¨¡å‹
    participant Buffer as ç¶“é©—ç·©è¡å€
    
    Player->>GameEngine: åˆå§‹åŒ–éŠæˆ²
    loop è¨“ç·´ Episode
        GameEngine->>GameEngine: é‡ç½®éŠæˆ²ç‹€æ…‹
        loop æ¯ä¸€å¹€
            GameEngine->>AI: ç™¼é€ç•¶å‰ç‹€æ…‹ (S)
            AI->>Model: æŸ¥è©¢ Q å€¼
            Model-->>AI: è¿”å› Q(S, A)
            AI->>AI: Îµ-greedy é¸æ“‡å‹•ä½œ
            AI-->>GameEngine: åŸ·è¡Œå‹•ä½œ (A)
            GameEngine->>GameEngine: è¨ˆç®—çå‹µ (R)
            GameEngine->>GameEngine: æ›´æ–°éŠæˆ²ç‹€æ…‹ (S')
            GameEngine->>Buffer: å­˜å„² (S, A, R, S', Done)
            alt éŠæˆ²æœªçµæŸ
                GameEngine->>RLTrainer: ç™¼é€çå‹µèˆ‡ç‹€æ…‹
            else éŠæˆ²çµæŸ
                GameEngine->>RLTrainer: ç™¼é€çµ‚æ­¢ä¿¡è™Ÿ
            end
        end
        RLTrainer->>Buffer: æ¡æ¨£ Mini-batch
        Buffer-->>RLTrainer: è¿”å›è¨“ç·´æ¨£æœ¬
        RLTrainer->>Model: è¨ˆç®— TD èª¤å·®
        RLTrainer->>Model: åå‘å‚³æ’­æ›´æ–°
        Model-->>RLTrainer: æ¨¡å‹æ›´æ–°å®Œæˆ
    end
    RLTrainer-->>Player: è¨“ç·´å®Œæˆï¼Œä¿å­˜æ¨¡å‹
```

---

## è¨­è¨ˆï¼šMSC åœ– - æ¨ç†æµç¨‹

```mermaid
sequenceDiagram
    participant Player as ç©å®¶
    participant GameEngine as éŠæˆ²å¼•æ“
    participant AI as AI æ±ºç­–æ¨¡çµ„
    participant Model as é è¨“ç·´æ¨¡å‹
    
    Player->>GameEngine: å•Ÿå‹•éŠæˆ²
    GameEngine->>GameEngine: åˆå§‹åŒ–ç‹€æ…‹
    loop éŠæˆ²é€²è¡Œä¸­
        GameEngine->>AI: ç™¼é€ç‹€æ…‹ (S)
        AI->>Model: æŸ¥è©¢æœ€ä½³å‹•ä½œ Q-value
        Model-->>AI: è¿”å› argmax_a Q(S, a)
        AI-->>GameEngine: è¿”å›å‹•ä½œ (A)
        GameEngine->>GameEngine: åŸ·è¡Œå‹•ä½œ
        GameEngine->>GameEngine: æ›´æ–°ç‰©ç†ã€çƒä½ç½®
        GameEngine->>GameEngine: æª¢æŸ¥å¾—åˆ†/ç¢°æ’
        alt éŠæˆ²ç¹¼çºŒ
            GameEngine-->>Player: æ¸²æŸ“ç•«é¢
        else éŠæˆ²çµæŸ
            GameEngine-->>Player: é¡¯ç¤ºæœ€çµ‚å¾—åˆ†
        end
    end
    Player->>Player: éŠæˆ²çµæŸ
```

---

## è¨­è¨ˆï¼šç³»çµ±æ¶æ§‹åœ–

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ä¹’ä¹“çƒéŠæˆ²ç³»çµ±æ¶æ§‹                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  éŠæˆ²å¼•æ“      â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”¤  AI æ±ºç­–æ¨¡çµ„    â”‚         â”‚
â”‚  â”‚  â€¢ ç‰©ç†æ¨¡æ“¬    â”‚         â”‚  â€¢ DQN æ¨¡å‹     â”‚         â”‚
â”‚  â”‚  â€¢ ç‹€æ…‹ç®¡ç†    â”‚         â”‚  â€¢ å‹•ä½œé¸æ“‡     â”‚         â”‚
â”‚  â”‚  â€¢ ç¢°æ’æª¢æ¸¬    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â€¢ ç‰¹å¾µæå–     â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚           â–²                           â–³                  â”‚
â”‚           â”‚                           â”‚                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚                  â”‚
â”‚  â”‚      è¨“ç·´æ¨¡çµ„                    â”‚â”‚                  â”‚
â”‚  â”‚  â€¢ RL è¨“ç·´è¿´åœˆ                   â”‚â”‚                  â”‚
â”‚  â”‚  â€¢ ç¶“é©—å›æ”¾ (Experience Replay)  â”‚â”‚                  â”‚
â”‚  â”‚  â€¢ ç›®æ¨™ç¶²çµ¡æ›´æ–°                  â”‚â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚                  â”‚
â”‚           â–³                           â”‚                  â”‚
â”‚           â”‚                           â–¼                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”              â”‚
â”‚  â”‚ æ•¸æ“šç®¡ç†æ¨¡çµ„       â”‚  è¦–è¦ºåŒ–æ¨¡çµ„       â”‚              â”‚
â”‚  â”‚ â€¢ ç¶“é©—ç·©è¡å€      â”‚  â€¢ UI æ¸²æŸ“       â”‚              â”‚
â”‚  â”‚ â€¢ æ¨¡å‹å­˜å„²        â”‚  â€¢ è¨“ç·´ç›£æ§      â”‚              â”‚
â”‚  â”‚ â€¢ æ•¸æ“šè¼‰å…¥        â”‚  â€¢ çµæœå±•ç¤º      â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜              â”‚
â”‚           â”‚                           â”‚                  â”‚
â”‚           â–¼                           â–¼                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚      å­˜å„²èˆ‡å¤–éƒ¨ç³»çµ±æ¥å£                 â”‚             â”‚
â”‚  â”‚  â€¢ ç£ç›¤å­˜å„² (æ¨¡å‹ã€æ—¥èªŒ)               â”‚             â”‚
â”‚  â”‚  â€¢ é…ç½®æ–‡ä»¶                            â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## è¨­è¨ˆï¼šæŠ€è¡“é¸å‹

| å±¤æ¬¡ | æŠ€è¡“ | åŸå›  |
|------|------|------|
| **éŠæˆ²å¼•æ“** | Pygame / è‡ªå»ºå¼•æ“ | ç°¡æ½”ã€æ˜“æ–¼é›†æˆ ML |
| **ML æ¡†æ¶** | TensorFlow/PyTorch | æˆç†Ÿã€æ”¯æ´ DQN |
| **ç®—æ³•** | Deep Q-Network (DQN) | é›¢æ•£å‹•ä½œç©ºé–“é©åˆ |
| **GPU/TPU** | CUDA (NVIDIA) | åŠ é€Ÿè¨“ç·´ |
| **èªè¨€** | Python | è±å¯Œçš„ ML ç”Ÿæ…‹ |

---

## è¨­è¨ˆï¼šDQN æ¨¡å‹çµæ§‹

```
è¼¸å…¥å±¤ (Input)
    â”‚ 4 ç¶­åº¦ (çƒä½ç½®x, çƒä½ç½®y, æ‹ä½ç½®x, æ‹é€Ÿåº¦)
    â–¼
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
éš±è—å±¤ 1 (Dense)
    â”‚ 64 neurons, ReLU æ¿€æ´»
    â–¼
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
éš±è—å±¤ 2 (Dense)
    â”‚ 64 neurons, ReLU æ¿€æ´»
    â–¼
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
éš±è—å±¤ 3 (Dense)
    â”‚ 32 neurons, ReLU æ¿€æ´»
    â–¼
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
è¼¸å‡ºå±¤ (Output)
    â”‚ 3 neurons (å·¦ç§»ã€ä¸å‹•ã€å³ç§»)
    â”‚ Linear æ¿€æ´»
    â–¼
Q å€¼è¼¸å‡º: [Q(s, left), Q(s, stay), Q(s, right)]
```

---

# ç¬¬å››éƒ¨åˆ†ï¼šç·¨ç¢¼å¯¦ç¾

---

## Codingï¼šæ ¸å¿ƒæ¨¡çµ„å¯¦ç¾

### 1. éŠæˆ²å¼•æ“æ¨¡çµ„

```python
# game_engine.py
import pygame
import numpy as np

class PingPongGame:
    def __init__(self, width=800, height=600):
        self.width = width
        self.height = height
        self.ball = {"x": width/2, "y": height/2, "vx": 5, "vy": 5}
        self.paddle = {"x": width/2, "y": height-50, "width": 100}
        self.score = 0
        self.done = False
    
    def get_state(self):
        """è¿”å›éŠæˆ²ç‹€æ…‹å‘é‡ [ball_x, ball_y, ball_vx, ball_vy, paddle_x]"""
        return np.array([
            self.ball["x"] / self.width,
            self.ball["y"] / self.height,
            self.ball["vx"] / 15,
            self.ball["vy"] / 15,
            self.paddle["x"] / self.width
        ], dtype=np.float32)
    
    def step(self, action):
        """åŸ·è¡Œå‹•ä½œ: 0=å·¦ç§», 1=ä¸å‹•, 2=å³ç§»"""
        # æ›´æ–°çƒä½ç½®
        self.ball["x"] += self.ball["vx"]
        self.ball["y"] += self.ball["vy"]
        
        # è™•ç†ç¢°æ’
        self._handle_collision()
        
        # æ›´æ–°çƒæ‹ä½ç½®
        if action == 0:  # å·¦ç§»
            self.paddle["x"] = max(0, self.paddle["x"] - 10)
        elif action == 2:  # å³ç§»
            self.paddle["x"] = min(self.width - self.paddle["width"], 
                                   self.paddle["x"] + 10)
        
        # è¨ˆç®—çå‹µ
        reward = self._compute_reward()
        
        return self.get_state(), reward, self.done
    
    def _handle_collision(self):
        """æª¢æ¸¬ç¢°æ’"""
        # çƒèˆ‡ç‰†çš„ç¢°æ’
        if self.ball["x"] <= 0 or self.ball["x"] >= self.width:
            self.ball["vx"] *= -1
        if self.ball["y"] <= 0:
            self.ball["vy"] *= -1
        
        # çƒèˆ‡çƒæ‹çš„ç¢°æ’
        if (self.height - 60 < self.ball["y"] < self.height - 40 and
            self.paddle["x"] < self.ball["x"] < self.paddle["x"] + self.paddle["width"]):
            self.ball["vy"] *= -1
            self.score += 1
        
        # éŠæˆ²çµæŸæ¢ä»¶
        if self.ball["y"] > self.height:
            self.done = True
    
    def _compute_reward(self):
        """è¨ˆç®—çå‹µ"""
        if self.score > 0:
            return 1.0  # æ¥åˆ°çƒç²å¾—æ­£çå‹µ
        elif self.done:
            return -10.0  # éŠæˆ²çµæŸçµ¦äºˆæ‡²ç½°
        else:
            return -0.01  # æ¯ä¸€æ­¥è¼•å¾®æ‡²ç½°ï¼Œä¿ƒé€²å¿«é€ŸéŠæˆ²
```

---

## Codingï¼šAI æ±ºç­–æ¨¡çµ„

```python
# dqn_agent.py
import tensorflow as tf
import numpy as np
from collections import deque
import random

class DQNAgent:
    def __init__(self, state_size=5, action_size=3):
        self.state_size = state_size
        self.action_size = action_size
        self.memory = deque(maxlen=2000)
        
        # è¶…åƒæ•¸
        self.gamma = 0.95  # æŠ˜æ‰£å› å­
        self.epsilon = 1.0  # æ¢ç´¢ç‡
        self.epsilon_decay = 0.995
        self.epsilon_min = 0.01
        self.learning_rate = 0.001
        
        # ç¥ç¶“ç¶²çµ¡
        self.model = self._build_model()
        self.target_model = self._build_model()
        self.update_target_model()
    
    def _build_model(self):
        """æ§‹å»º DQN æ¨¡å‹"""
        model = tf.keras.Sequential([
            tf.keras.layers.Dense(64, input_dim=self.state_size, activation='relu'),
            tf.keras.layers.Dense(64, activation='relu'),
            tf.keras.layers.Dense(32, activation='relu'),
            tf.keras.layers.Dense(self.action_size, activation='linear')
        ])
        model.compile(optimizer=tf.keras.optimizers.Adam(lr=self.learning_rate),
                      loss='mse')
        return model
    
    def update_target_model(self):
        """æ›´æ–°ç›®æ¨™ç¶²çµ¡"""
        self.target_model.set_weights(self.model.get_weights())
    
    def remember(self, state, action, reward, next_state, done):
        """å­˜å„²ç¶“é©—åˆ°é‡æ”¾ç·©è¡å€"""
        self.memory.append((state, action, reward, next_state, done))
    
    def act(self, state):
        """ä½¿ç”¨ Îµ-greedy ç­–ç•¥é¸æ“‡å‹•ä½œ"""
        if np.random.random() < self.epsilon:
            return random.randrange(self.action_size)  # æ¢ç´¢
        q_values = self.model.predict(state[np.newaxis, :], verbose=0)
        return np.argmax(q_values[0])  # é–‹ç™¼
    
    def replay(self, batch_size):
        """å¾ç¶“é©—ç·©è¡å€æŠ½æ¨£ä¸¦è¨“ç·´"""
        if len(self.memory) < batch_size:
            return
        
        batch = random.sample(self.memory, batch_size)
        states = np.array([x[0] for x in batch])
        actions = np.array([x[1] for x in batch])
        rewards = np.array([x[2] for x in batch])
        next_states = np.array([x[3] for x in batch])
        dones = np.array([x[4] for x in batch])
        
        # Bellman æ–¹ç¨‹
        target_q = self.model.predict(states, verbose=0)
        next_q = self.target_model.predict(next_states, verbose=0)
        
        for i in range(batch_size):
            if dones[i]:
                target_q[i][actions[i]] = rewards[i]
            else:
                target_q[i][actions[i]] = rewards[i] + self.gamma * np.max(next_q[i])
        
        self.model.fit(states, target_q, epochs=1, verbose=0)
        
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
```

---

## Codingï¼šè¨“ç·´è¿´åœˆ

```python
# train.py
from game_engine import PingPongGame
from dqn_agent import DQNAgent

def train_dqn(episodes=1000, batch_size=32):
    """è¨“ç·´ DQN ä»£ç†"""
    game = PingPongGame()
    agent = DQNAgent()
    
    episode_rewards = []
    
    for episode in range(episodes):
        state = game.get_state()
        game.reset()
        total_reward = 0
        
        while not game.done:
            action = agent.act(state)
            next_state, reward, done = game.step(action)
            
            agent.remember(state, action, reward, next_state, done)
            agent.replay(batch_size)
            
            total_reward += reward
            state = next_state
        
        if (episode + 1) % 10 == 0:
            agent.update_target_model()
            episode_rewards.append(total_reward)
            print(f"Episode {episode+1}/{episodes}, Reward: {total_reward:.2f}")
    
    agent.model.save('dqn_pingpong.h5')
    return episode_rewards
```

---

# ç¬¬äº”éƒ¨åˆ†ï¼šé©—è­‰èˆ‡æ¸¬è©¦

---

## é©—è­‰ï¼šæ¸¬è©¦è¨ˆåŠƒ

| æ¸¬è©¦é¡å‹ | æ¸¬è©¦é …ç›® | æœŸæœ›çµæœ |
|---------|---------|---------|
| **å–®å…ƒæ¸¬è©¦** | éŠæˆ²ç‰©ç†æ¨¡æ“¬ | çƒé‹å‹•è»Œè·¡æ­£ç¢º |
| **å–®å…ƒæ¸¬è©¦** | ç¢°æ’æª¢æ¸¬ | é‚Šç•Œå’Œçƒæ‹ç¢°æ’æº–ç¢º |
| **é›†æˆæ¸¬è©¦** | AI æ±ºç­–èˆ‡éŠæˆ²äº¤äº’ | ç„¡å»¶é²ã€å‹•ä½œåŸ·è¡Œæ­£ç¢º |
| **æ€§èƒ½æ¸¬è©¦** | æ¨ç†é€Ÿåº¦ | <30ms/frame |
| **æ€§èƒ½æ¸¬è©¦** | è¨“ç·´æ•ˆç‡ | æ”¶æ–‚åœ¨ 1000 episodes å…§ |

---

## é©—è­‰ï¼šæ¨¡å‹è©•ä¼°æŒ‡æ¨™

### è¨“ç·´æŒ‡æ¨™

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        è¨“ç·´é€²åº¦ç›£æ§æŒ‡æ¨™              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. å¹³å‡çå‹µ (Moving Average)         â”‚
â”‚    âœ“ è¶¨å‹¢: æ‡‰é€æ­¥ä¸Šå‡                â”‚
â”‚                                      â”‚
â”‚ 2. æå¤±å‡½æ•¸ (MSE Loss)               â”‚
â”‚    âœ“ è¶¨å‹¢: æ‡‰é€æ­¥ä¸‹é™                â”‚
â”‚                                      â”‚
â”‚ 3. æ¢ç´¢ç‡ (Epsilon)                  â”‚
â”‚    âœ“ è¶¨å‹¢: æŒ‡æ•¸è¡°æ¸›                  â”‚
â”‚                                      â”‚
â”‚ 4. Q å€¼ä¼°è¨ˆ                          â”‚
â”‚    âœ“ ç¯„åœ: æ‡‰åœ¨åˆç†ç¯„åœå…§            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## é©—è­‰ï¼šæ¸¬è©¦çµæœè©•ä¼°

### æ¥çƒæˆåŠŸç‡è©•ä¼°

```
æ¸¬è©¦æ¢ä»¶: 100 å±€éŠæˆ²ï¼Œçƒé€Ÿåº¦ = 10 px/frame
ç›®æ¨™å€¼: â‰¥85%

çµæœç¤ºä¾‹:
Episode 1-10:   å¹³å‡å‘½ä¸­ç‡ 65%  âŒ (æœªé”æ¨™)
Episode 101-110: å¹³å‡å‘½ä¸­ç‡ 78%  âš ï¸ (æ¥è¿‘)
Episode 501-510: å¹³å‡å‘½ä¸­ç‡ 88%  âœ“ (é”æ¨™)
Episode 951-1000: å¹³å‡å‘½ä¸­ç‡ 92%  âœ“âœ“ (è¶…é¡)

çµè«–: æ¨¡å‹åœ¨ç¬¬ 500+ episode å¾Œç©©å®šé”åˆ° 85% ä»¥ä¸Š
```

---

## é©—è­‰ï¼šæ¸¬è©¦å ±å‘Šç¯„æœ¬

### æ€§èƒ½åŸºæº–æ¸¬è©¦

| æ¸¬è©¦é …ç›® | ç›®æ¨™å€¼ | å¯¦æ¸¬å€¼ | ç‹€æ…‹ |
|---------|--------|--------|------|
| æ¥çƒæˆåŠŸç‡ | â‰¥85% | 92% | âœ“ PASS |
| å¹³å‡éŠæˆ²æ™‚é•· | â‰¥100 çƒ | 156 çƒ | âœ“ PASS |
| æ¨ç†å»¶é² | <30 ms | 18 ms | âœ“ PASS |
| è¨“ç·´æ™‚é–“ | â‰¤2 å°æ™‚ | 1.5 å°æ™‚ | âœ“ PASS |
| ç©©å®šæ€§ | Ïƒ<10% | Ïƒ=7.3% | âœ“ PASS |

---

## é©—è­‰ï¼šèª¿è©¦èˆ‡å„ªåŒ–

### å¸¸è¦‹å•é¡Œæ’æŸ¥è¡¨

| å•é¡Œ | ç—‡ç‹€ | è§£æ±ºæ–¹æ¡ˆ |
|------|------|---------|
| è¨“ç·´ä¸æ”¶æ–‚ | çå‹µç„¡é€²å±• | èª¿æ•´å­¸ç¿’ç‡ã€çå‹µè¨­è¨ˆ |
| éåº¦æ“¬åˆ | è¨“ç·´é›†å¼·ï¼Œæ¸¬è©¦å¼± | å¢åŠ ç¶“é©—å›æ”¾å¤šæ¨£æ€§ |
| æ¨ç†å»¶é²éé«˜ | fps <30 | ç°¡åŒ–æ¨¡å‹æˆ–ä½¿ç”¨é‡åŒ– |
| é–“æ­‡æ€§å¤±æ•— | æ™‚è€ŒæˆåŠŸæ™‚è€Œå¤±æ•— | å¢åŠ è¨“ç·´ episodes |
| æ¨¡å‹ä¸ç©©å®š | æ€§èƒ½æ³¢å‹•å¤§ | å¢åŠ ç›®æ¨™ç¶²çµ¡æ›´æ–°å‘¨æœŸ |

---

# çµè«–èˆ‡å¾ŒçºŒå·¥ä½œ

---

## é …ç›®æˆæœç¸½çµ

### é”æˆç›®æ¨™

âœ“ å»ºç«‹å®Œæ•´çš„ä¹’ä¹“çƒéŠæˆ²ç’°å¢ƒ  
âœ“ å¯¦ç¾ DQN å¼·åŒ–å­¸ç¿’ç®—æ³•  
âœ“ è¨“ç·´æ¨¡å‹é”åˆ° 85%+ æ¥çƒæˆåŠŸç‡  
âœ“ é©—è­‰ç³»çµ±æ€§èƒ½æŒ‡æ¨™  

### æŠ€è¡“äº®é»

â€¢ ç«¯åˆ°ç«¯æ·±åº¦å¼·åŒ–å­¸ç¿’ç³»çµ±  
â€¢ ç¶“é©—å›æ”¾èˆ‡ç›®æ¨™ç¶²çµ¡å„ªåŒ–  
â€¢ Bellman æ–¹ç¨‹çš„å¯¦éš›æ‡‰ç”¨  
â€¢ å®Œæ•´çš„æ¸¬è©¦èˆ‡è©•ä¼°æ¡†æ¶  

---

## æœªä¾†å·¥ä½œæ–¹å‘

### çŸ­æœŸæ”¹é€² (1-2 å€‹æœˆ)

- [ ] æ¨¡å‹é‡åŒ–ä»¥æå‡æ¨ç†é€Ÿåº¦
- [ ] å¢åŠ å¤šå±¤æ¬¡é›£åº¦è¨­ç½®
- [ ] å¯¦ç¾ç©å®¶å°æˆ° AI çš„å®Œæ•´ UI
- [ ] è¶…åƒæ•¸è‡ªå‹•èª¿å„ª

### ä¸­æœŸæ“´å±• (2-6 å€‹æœˆ)

- [ ] é·ç§»åˆ° TetrAI (ä¿„ç¾…æ–¯æ–¹å¡Š) å°ˆæ¡ˆ
- [ ] å¯¦ç¾å¤šä»£ç†ç«¶çˆ­å­¸ç¿’
- [ ] é©—è­‰é·ç§»å­¸ç¿’æ•ˆæœ

### é•·æœŸé¡˜æ™¯ (6+ æœˆ)

- [ ] æ”¯æ´è¤‡é›œéŠæˆ²ç’°å¢ƒ
- [ ] çµåˆäººé¡åé¥‹çš„å¼·åŒ–å­¸ç¿’
- [ ] ç™¼è¡¨ç›¸é—œç ”ç©¶è«–æ–‡

---

## åƒè€ƒè³‡æº

### è«–æ–‡èˆ‡æ–‡ç»

1. Mnih et al. (2015). Human-level control through deep reinforcement learning
2. Van Hasselt et al. (2016). Deep Reinforcement Learning with Double Q-learning
3. Schaul et al. (2016). Prioritized Experience Replay

### é–‹æºæ¡†æ¶

- TensorFlow / PyTorch
- OpenAI Gym
- Stable Baselines3

### ç›¸é—œå°ˆæ¡ˆ

- TetrAI (ä¿„ç¾…æ–¯æ–¹å¡Š AI)
- OpenAI Five (Dota 2)
- AlphaGo ç³»åˆ—

---

# è¬è¬è†è½

### è¯çµ¡æ–¹å¼

ğŸ“§ Email: your.email@example.com  
ğŸ”— GitHub: github.com/yourname/pingpong-ai  
ğŸ“š Documentation: [å°ˆæ¡ˆ Wiki]

---

