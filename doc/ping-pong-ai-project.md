# æ©Ÿå™¨å­¸ç¿’å°ˆé¡Œ - ä¹’ä¹“çƒéŠæˆ² AI ç³»çµ±

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

åŸºæ–¼ Deep Q-Network (DQN) çš„ä¹’ä¹“çƒéŠæˆ² AI è¨“ç·´ç³»çµ±,å¯¦ç¾è‡ªä¸»å­¸ç¿’èˆ‡æ±ºç­–ã€‚

---

## ğŸ“‹ ç›®éŒ„

- [éœ€æ±‚åˆ†æ](#-éœ€æ±‚åˆ†æ)
- [ç³»çµ±åˆ†æ](#-ç³»çµ±åˆ†æ)
- [ç³»çµ±è¨­è¨ˆ](#-ç³»çµ±è¨­è¨ˆ)
- [ç·¨ç¢¼å¯¦ç¾](#-ç·¨ç¢¼å¯¦ç¾)
- [å¾…æ¸¬è©¦èˆ‡æ¸¬è©¦](#-å¾…æ¸¬è©¦èˆ‡æ¸¬è©¦)
- [åƒè€ƒè³‡æº](#-åƒè€ƒè³‡æº)

---

## ğŸ¯ éœ€æ±‚åˆ†æ

### 1.1 åŠŸèƒ½æ€§éœ€æ±‚

| ID | åŠŸèƒ½æè¿° | å„ªå…ˆç´š |
|:---|:---------|:------:|
| F1 | éŠæˆ²ç’°å¢ƒå»ºç«‹èˆ‡åˆå§‹åŒ– | P0 |
| F2 | çƒç‰©ç†é‹å‹•æ¨¡æ“¬ | P0 |
| F3 | çƒæ‹æ§åˆ¶èˆ‡ç§»å‹• | P0 |
| F4 | AI æ±ºç­–èˆ‡å‹•ä½œåŸ·è¡Œ | P0 |
| F5 | éŠæˆ²ç‹€æ…‹è·Ÿè¹¤èˆ‡å¾—åˆ†è¨ˆç®— | P1 |
| F6 | è¨“ç·´/é æ¸¬æ¨¡å¼åˆ‡æ› | P1 |
| F7 | æ¨¡å‹ä¿å­˜èˆ‡åŠ è¼‰ | P1 |
| F8 | è¦–è¦ºåŒ–çµæœå‘ˆç¾ | P2 |

### 1.2 è¦æ ¼éœ€æ±‚

```yaml
éŠæˆ²å¼•æ“:
  å¹€ç‡: 60 FPS
  çƒé€Ÿåº¦ç¯„åœ: 5~15 px/frame
  çƒæ‹åæ‡‰æ™‚é–“: â‰¤50 ms

AI æ¨¡å‹:
  è¼¸å…¥ç‹€æ…‹ç¶­åº¦: 5 [ball_x, ball_y, vx, vy, paddle_x]
  è¼¸å‡ºå‹•ä½œç©ºé–“: 3 [å·¦ç§», ä¸å‹•, å³ç§»]
  æ¨ç†å»¶é²: <30 ms/action

è¨“ç·´é…ç½®:
  è¨“ç·´æ”¶æ–‚æ™‚é–“: â‰¤2å°æ™‚ (1000 episodes)
  è¨˜æ†¶é«”éœ€æ±‚: â‰¤2 GB
  æ”¶æ–‚æ¨™æº–: 500 episodes
```

### 1.3 æ•ˆèƒ½éœ€æ±‚

| æŒ‡æ¨™ | ç›®æ¨™å€¼ | é©—æ”¶æ¨™æº– |
|:-----|:-------|:---------|
| æ¥çƒæˆåŠŸç‡ | â‰¥85% | é€£çºŒ 100 å±€æ¸¬è©¦ |
| å¹³å‡éŠæˆ²æ™‚é•· | â‰¥100 çƒ | å–®å±€çµ±è¨ˆ |
| æ¨¡å‹ç©©å®šæ€§ | Ïƒ <10% | æ¨™æº–å·®è¨ˆç®— |
| æ¨ç†å»¶é² | <30 ms | å–®æ¬¡å‹•ä½œæ±ºç­– |
| è¨“ç·´æ”¶æ–‚ | 500 episodes | Loss æ›²ç·šç©©å®š |

### 1.4 é©—æ”¶æ–¹æ³•

```
é©—æ”¶æ¸¬è©¦é«”ç³»
â”‚
â”œâ”€ åŠŸèƒ½é©—æ”¶
â”‚  â”œâ”€ å–®å…ƒæ¸¬è©¦
â”‚  â”œâ”€ é›†æˆæ¸¬è©¦
â”‚  â””â”€ åŠŸèƒ½æª¢æŸ¥è¡¨
â”‚
â”œâ”€ æ€§èƒ½é©—æ”¶
â”‚  â”œâ”€ åŸºæº–æ¸¬è©¦
â”‚  â”œâ”€ å£“åŠ›æ¸¬è©¦
â”‚  â””â”€ è¨˜æ†¶é«”æ¸¬è©¦
â”‚
â”œâ”€ æ¨¡å‹é©—æ”¶
â”‚  â”œâ”€ æº–ç¢ºåº¦è©•ä¼°
â”‚  â”œâ”€ æ³›åŒ–æ€§æ¸¬è©¦
â”‚  â””â”€ ç©©å®šæ€§æ¸¬è©¦
â”‚
â””â”€ ç”¨æˆ¶é©—æ”¶
   â”œâ”€ æ–‡ä»¶å®Œæ•´æ€§
   â”œâ”€ è¦–è¦ºåŒ–çµæœ
   â””â”€ ä»£ç¢¼å¯è®€æ€§
```

---

## ğŸ“Š ç³»çµ±åˆ†æ

### 2.1 ç”¨ä¾‹åœ– (Use Case)

```mermaid
graph TB
    subgraph ç³»çµ±é‚Šç•Œ
        UC1[è¨“ç·´ AI æ¨¡å‹]
        UC2[åŸ·è¡Œå°æˆ°éŠæˆ²]
        UC3[è©•ä¼°æ¨¡å‹æ€§èƒ½]
        UC4[ä¿å­˜/åŠ è¼‰æ¨¡å‹]
        UC5[ç›£æ§è¨“ç·´é€²åº¦]
    end

    Developer[é–‹ç™¼è€…] -->|è¨“ç·´| UC1
    Developer -->|ä¿å­˜| UC4
    Developer -->|ç›£æ§| UC5

    Player[ç©å®¶] -->|å°æˆ°| UC2

    Researcher[ç ”ç©¶å“¡] -->|è©•ä¼°| UC3
    Researcher -->|åŠ è¼‰| UC4

    UC1 -.->|include| UC4
    UC1 -.->|include| UC5
    UC2 -.->|include| UC4
```

### 2.2 åƒæ•¸èˆ‡æå¤±å‡½æ•¸çš„å«ç¾©

#### 2.2.1 åƒæ•¸ Î¸ çš„å®šç¾©èˆ‡æ„ç¾©

DQN ç¥ç¶“ç¶²çµ¡çš„æ‰€æœ‰å¯å­¸ç¿’åƒæ•¸:

**åƒæ•¸é›†åˆ**:
$\theta = \{W_1, b_1, W_2, b_2, W_3, b_3, W_{out}, b_{out}\}$

**åƒæ•¸æ›´æ–°è¦å‰‡**:
$\theta_{new} = \theta_{old} - \alpha \cdot \nabla_\theta L(\theta)$

#### 2.2.1.1 DQN åƒæ•¸æŠ½è±¡åŒ–å®šç¾©è¡¨

| ç¬¦è™Ÿ | åƒæ•¸åç¨± | ç¶­åº¦ | æŠ½è±¡å®šç¾© | ç‰©ç†æ„ç¾© | åˆå§‹åŒ–æ–¹å¼ |
|:-----|:---------|:-----|:---------|:---------|:----------|
| **Wâ‚** | ç¬¬1å±¤æ¬Šé‡çŸ©é™£ | 5Ã—64 | ç‰¹å¾µæå–å™¨ | å°‡åŸå§‹ç‹€æ…‹å‘é‡æ˜ å°„åˆ°64ç¶­éš±ç©ºé–“ | Xavierå‡å‹» |
| **bâ‚** | ç¬¬1å±¤åç½®å‘é‡ | 64 | å¹³ç§»é … | èª¿æ•´ç¬¬1å±¤æ¿€æ´»çš„åŸºæº–é» | å…¨é›¶åˆå§‹åŒ– |
| **Wâ‚‚** | ç¬¬2å±¤æ¬Šé‡çŸ©é™£ | 64Ã—64 | ç‰¹å¾µèåˆå™¨ | åœ¨éš±ç©ºé–“ä¸­é€²è¡Œ64ç¶­ç·šæ€§è®Šæ› | Xavierå‡å‹» |
| **bâ‚‚** | ç¬¬2å±¤åç½®å‘é‡ | 64 | å¹³ç§»é … | èª¿æ•´ç¬¬2å±¤æ¿€æ´»çš„åŸºæº–é» | å…¨é›¶åˆå§‹åŒ– |
| **Wâ‚ƒ** | ç¬¬3å±¤æ¬Šé‡çŸ©é™£ | 64Ã—32 | ç‰¹å¾µå£“ç¸®å™¨ | å°‡64ç¶­ç‰¹å¾µå£“ç¸®åˆ°32ç¶­ | Xavierå‡å‹» |
| **bâ‚ƒ** | ç¬¬3å±¤åç½®å‘é‡ | 32 | å¹³ç§»é … | èª¿æ•´ç¬¬3å±¤æ¿€æ´»çš„åŸºæº–é» | å…¨é›¶åˆå§‹åŒ– |
| **W_{out}** | è¼¸å‡ºå±¤æ¬Šé‡çŸ©é™£ | 32Ã—3 | Qå€¼æ˜ å°„å™¨ | å°‡32ç¶­ç‰¹å¾µæ˜ å°„åˆ°3å€‹Qå€¼ | Xavierå‡å‹» |
| **b_{out}** | è¼¸å‡ºå±¤åç½®å‘é‡ | 3 | å¹³ç§»é … | èª¿æ•´Qå€¼çš„åŸºæº–ç·š | å…¨é›¶åˆå§‹åŒ– |

#### 2.2.1.2 æ¢¯åº¦èˆ‡å„ªåŒ–åƒæ•¸è¡¨

| ç¬¦è™Ÿ | åƒæ•¸åç¨± | å–å€¼ç¯„åœ | æŠ½è±¡å®šç¾© | å½±éŸ¿æ•ˆæœ |
|:-----|:---------|:----------|:---------|:---------|
| **Î±** | å­¸ç¿’ç‡ | 0.00025 | åƒæ•¸æ›´æ–°çš„æ­¥é•·æ§åˆ¶å› å­ | éå¤§â†’ä¸ç©©å®šï¼›éå°â†’æ”¶æ–‚æ…¢ |
| **âˆ‡Î¸L(Î¸)** | æ¢¯åº¦å‘é‡ | â„â¿ | æå¤±å‡½æ•¸åœ¨åƒæ•¸ç©ºé–“çš„æ–¹å‘å°æ•¸ | æŒ‡å‘æå¤±æ¸›å°æ–¹å‘ |
| **Î³** | æŠ˜æ‰£å› å­ | 0.99 | æœªä¾†çå‹µçš„è¡°æ¸›ä¿‚æ•¸ | Î³å¤§â†’é è¦–ï¼›Î³å°â†’è¿‘è¦– |
| **Îµ** | æ¢ç´¢ç‡ | [0.05, 1.0] | éš¨æ©Ÿæ¢ç´¢å‹•ä½œçš„æ¦‚ç‡ | Îµå¤§â†’æ¢ç´¢å¤šï¼›Îµå°â†’åˆ©ç”¨å¤š |
| **Ï„** | ç›®æ¨™ç¶²çµ¡æ›´æ–°ç‡ | 0.001 | è»Ÿæ›´æ–°æ™‚çš„æ··åˆä¿‚æ•¸ | æ§åˆ¶ç©©å®šæ€§èˆ‡é©æ‡‰é€Ÿåº¦ |

#### 2.2.1.3 å‹•ä½œèˆ‡ç‹€æ…‹ç©ºé–“åƒæ•¸è¡¨

| ç¬¦è™Ÿ | åƒæ•¸åç¨± | ç¶­åº¦ | æŠ½è±¡å®šç¾© | å…·é«”å…§å®¹ |
|:-----|:---------|:-----|:---------|:---------|
| **S** | ç‹€æ…‹å‘é‡ | 5 | éŠæˆ²ç’°å¢ƒçš„å®Œæ•´è§€å¯Ÿ | [ball_x, ball_y, vel_x, vel_y, paddle_x] |
| **A** | å‹•ä½œç©ºé–“ | 3 | æ™ºèƒ½é«”å¯åŸ·è¡Œçš„å‹•ä½œé›†åˆ | {0:å·¦ç§», 1:ä¸å‹•, 2:å³ç§»} |
| **R** | çå‹µå€¼ | 1 | åŸ·è¡Œå‹•ä½œå¾Œç’°å¢ƒçš„å³æ™‚åé¥‹ | {-1:å¤±åˆ†, 0:ç„¡, +1:å¾—åˆ†} |
| **Q(s,a)** | Qå€¼å‡½æ•¸ | 1 | åœ¨ç‹€æ…‹såŸ·è¡Œå‹•ä½œaçš„æœŸæœ›ç´¯ç©çå‹µ | Î¸åƒæ•¸åŒ–çš„ç¥ç¶“ç¶²çµ¡è¼¸å‡º |
| **V(s)** | ç‹€æ…‹åƒ¹å€¼å‡½æ•¸ | 1 | åœ¨ç‹€æ…‹sçš„æœ€å„ªé•·æœŸçå‹µæœŸæœ› | max_a Q(s,a) |

#### 2.2.1.4 è¨“ç·´è¶…åƒæ•¸è¡¨

| åƒæ•¸å | ç¬¦è™Ÿ | é è¨­å€¼ | ç¯„åœ | ä½œç”¨åŸŸ | èª¿æ•´å»ºè­° |
|:-------|:-----|:--------|:-----|:--------|:---------|
| å­¸ç¿’ç‡ | Î± | 0.001 | [0.0001, 0.1] | æ¬Šé‡æ›´æ–°é€Ÿåº¦ | è¨“ç·´ä¸ç©©å®šâ†’é™ä½ |
| æŠ˜æ‰£å› å­ | Î³ | 0.99 | [0.9, 0.999] | é•·æœŸçå‹µæ¬Šé‡ | çŸ­æœŸç­–ç•¥â†’é™ä½ |
| æ‰¹æ¬¡å¤§å° | B | 64 | [32, 256] | æ¢¯åº¦ä¼°è¨ˆ | è¨˜æ†¶é«”ä¸è¶³â†’é™ä½ |
| ç·©è¡å®¹é‡ | M | 10000 | [1000, 100000] | ç¶“é©—å›æ”¾ | è¨“ç·´ä¸ç©©å®šâ†’å¢åŠ  |
| Îµåˆå§‹å€¼ | Îµâ‚€ | 1.0 | [0.8, 1.0] | åˆå§‹æ¢ç´¢ | å›ºå®š |
| Îµæœ€å°å€¼ | Îµ_min | 0.05 | [0.01, 0.1] | æœ€ä½æ¢ç´¢ç‡ | å–æ±ºæ–¼ç’°å¢ƒ |
| Îµè¡°æ¸›ç‡ | Îµ_decay | 0.995 | [0.99, 0.9999] | æ¢ç´¢è¡°æ¸›é€Ÿåº¦ | è¡°æ¸›å¤ªå¿«â†’æé«˜ |
| æ›´æ–°é »ç‡ | Ï„_freq | 10 | [5, 100] | ç›®æ¨™ç¶²çµ¡åŒæ­¥ | è¨“ç·´éœ‡è•©â†’å¢åŠ  |

å…¶ä¸­:
- **Î± (å­¸ç¿’ç‡)**: 0.001 - æ§åˆ¶æ¯æ¬¡æ›´æ–°çš„æ­¥é•·å¤§å°
- **âˆ‡Î¸L(Î¸)**: æå¤±å‡½æ•¸å°åƒæ•¸çš„æ¢¯åº¦ - æŒ‡ç¤ºåƒæ•¸æ›´æ–°çš„æ–¹å‘

#### 2.2.2 æå¤±å‡½æ•¸ L(Î¸) çš„çµæ§‹èˆ‡æ¼”é€²

**DQN æå¤±å‡½æ•¸**:
$$L(\theta) = \mathbb{E}\left[(Q_{target}(S,A) - Q_\theta(S,A))^2\right]$$

å…¶ä¸­ç›®æ¨™ Q å€¼è¨ˆç®—:
$$Q_{target} = r + \gamma \cdot \max_{a'} Q_{\theta^-}(S', a')$$

**è¨“ç·´éšæ®µæ¼”è®Š**:

| éšæ®µ | Episodes | Loss ç¯„åœ | Îµ å€¼ | ç‰¹å¾µ |
|:-----|:---------|:----------|:-----|:-----|
| åˆå§‹åŒ– | 0-100 | é«˜ä¸”ä¸ç©©å®š | 1.0â†’0.8 | éš¨æ©Ÿæ¢ç´¢ç‚ºä¸» |
| å­¸ç¿’æœŸ | 100-500 | é€æ¼¸ä¸‹é™ | 0.8â†’0.3 | é–‹å§‹å­¸ç¿’æœ‰æ•ˆç­–ç•¥ |
| æ”¶æ–‚æœŸ | 500-1000 | ç©©å®šä½å€¼ | 0.3â†’0.05 | ç­–ç•¥å„ªåŒ– |
| ç©©å®šæœŸ | 1000+ | å¹³ç©© | 0.05 | é«˜åº¦åˆ©ç”¨å­¸ç¿’çµæœ |

#### 2.2.3 Î¸ åœ¨éŠæˆ²æ±ºç­–ä¸­çš„å…·é«”æ©Ÿåˆ¶

**ç‹€æ…‹ â†’ Q å€¼è½‰æ›éç¨‹**:

```
è¼¸å…¥ç‹€æ…‹
  â”‚
  â”œâ”€ ball_x (çƒçš„ X åº§æ¨™)
  â”œâ”€ ball_y (çƒçš„ Y åº§æ¨™)
  â”œâ”€ vx (çƒçš„ X æ–¹å‘é€Ÿåº¦)
  â”œâ”€ vy (çƒçš„ Y æ–¹å‘é€Ÿåº¦)
  â””â”€ paddle_x (çƒæ‹çš„ X åº§æ¨™)
  â”‚
  â”œâ”€ ç¬¬ä¸€å±¤æ¬Šé‡ W1, b1 (5â†’64)
  â”‚  â””â”€ åŸå§‹ç‹€æ…‹ç‰¹å¾µæå–
  â”‚
  â”œâ”€ ç¬¬äºŒå±¤æ¬Šé‡ W2, b2 (64â†’64)
  â”‚  â””â”€ ä¸­å±¤ç‰¹å¾µçµ„åˆèˆ‡èåˆ
  â”‚
  â”œâ”€ ç¬¬ä¸‰å±¤æ¬Šé‡ W3, b3 (64â†’32)
  â”‚  â””â”€ é«˜å±¤æŠ½è±¡è¡¨ç¤º
  â”‚
  â””â”€ è¼¸å‡ºå±¤æ¬Šé‡ Wout, bout (32â†’3)
     â””â”€ å‹•ä½œåƒ¹å€¼æ˜ å°„
        â”‚
        â”œâ”€ Q (å·¦ç§»)
        â”œâ”€ Q (ä¸å‹•)
        â””â”€ Q (å³ç§»)
        â”‚
        â””â”€ argmax(Q) â†’ é¸æ“‡å‹•ä½œ
```

---

## ğŸ—ï¸ ç³»çµ±è¨­è¨ˆ

### 3.1 ç³»çµ±æ¨¡çµ„åˆ†æ”¯åœ–

```mermaid
graph TB
    Root[ä¹’ä¹“çƒ AI ç³»çµ±] --> M1[ğŸ® éŠæˆ²å¼•æ“æ¨¡çµ„]
    Root --> M2[ğŸ¤– AI æ±ºç­–æ¨¡çµ„]
    Root --> M3[ğŸ“š è¨“ç·´æ¨¡çµ„]
    Root --> M4[ğŸ’¾ æ•¸æ“šç®¡ç†æ¨¡çµ„]
    Root --> M5[ğŸ¨ è¦–è¦ºåŒ–æ¨¡çµ„]

    M1 --> M11[ç‰©ç†å¼•æ“]
    M1 --> M12[ç¢°æ’åµæ¸¬]
    M1 --> M13[ç‹€æ…‹ç®¡ç†]

    M2 --> M21[DQN æ¨¡å‹]
    M2 --> M22[Îµ-greedy é¸æ“‡]
    M2 --> M23[å‹•ä½œåŸ·è¡Œ]

    M3 --> M31[RL è¨“ç·´è¿´åœˆ]
    M3 --> M32[çå‹µè¨ˆç®—]
    M3 --> M33[ç¶“é©—å›æ”¾]

    M4 --> M41[ç¶“é©—ç·©è¡å€]
    M4 --> M42[æ¨¡å‹å­˜å„²]
    M4 --> M43[æª¢æŸ¥é»ç®¡ç†]

    M5 --> M51[UI æ¸²æŸ“]
    M5 --> M52[è¨“ç·´ç›£æ§]
    M5 --> M53[çµæœå±•ç¤º]
```

### 3.2 è³‡æ–™æµåœ– (Data Flow Diagram)

#### Level 0: ç³»çµ±è„ˆçµ¡åœ–

```mermaid
graph LR
    Player[ç©å®¶/é–‹ç™¼è€…] -->|è¼¸å…¥æŒ‡ä»¤| System[ä¹’ä¹“çƒ AI ç³»çµ±]
    System -->|éŠæˆ²ç•«é¢/è¨“ç·´çµæœ| Player
    System -->|æ¨¡å‹æ¬Šé‡| Storage[(æ¨¡å‹å­˜å„²)]
    Storage -->|åŠ è¼‰æ¨¡å‹| System
```

#### Level 1: ä¸»è¦æµç¨‹åœ–

```mermaid
graph TB
    Input[è¼¸å…¥å±¤] -->|éŠæˆ²ç‹€æ…‹| Process[è™•ç†å±¤]
    Input -->|è¨“ç·´æŒ‡ä»¤| Process
    Input -->|æ¨¡å‹æª”æ¡ˆ| Process

    Process -->|ç‹€æ…‹è™•ç†| P1[éŠæˆ²å¼•æ“]
    Process -->|æ±ºç­–è¨ˆç®—| P2[AI æ±ºç­–]
    Process -->|è¨“ç·´æ›´æ–°| P3[è¨“ç·´æ¨¡çµ„]

    P1 -->|æ–°ç‹€æ…‹| Output[è¼¸å‡ºå±¤]
    P2 -->|å‹•ä½œ| Output
    P3 -->|æ¨¡å‹åƒæ•¸| Output

    Output -->|è¦–è¦ºåŒ–| Display[é¡¯ç¤ºçµæœ]
    Output -->|ä¿å­˜| Storage[(æ•¸æ“šå­˜å„²)]
```

### 3.3 è¨“ç·´æµç¨‹åºåˆ—åœ– (Training MSC)

```mermaid
sequenceDiagram
    participant Dev as é–‹ç™¼è€…
    participant Env as éŠæˆ²å¼•æ“
    participant Agent as AI æ±ºç­–
    participant Train as è¨“ç·´æ¨¡çµ„
    participant Mem as ç¶“é©—ç·©è¡
    participant DQN as DQN æ¨¡å‹

    Dev->>Env: åˆå§‹åŒ–éŠæˆ²ç’°å¢ƒ
    Env-->>Dev: è¿”å›åˆå§‹ç‹€æ…‹ S0

    loop è¨“ç·´è¿´åœˆ (æ¯å€‹ Episode)
        Dev->>Agent: ç²å–å‹•ä½œ (ç‹€æ…‹ St)
        Agent->>DQN: forward(St)
        DQN-->>Agent: Q å€¼ [Q(å·¦), Q(ä¸å‹•), Q(å³)]
        Agent->>Agent: Îµ-greedy é¸æ“‡å‹•ä½œ At
        Agent-->>Env: åŸ·è¡Œå‹•ä½œ At

        Env->>Env: æ›´æ–°ç‰©ç†ç‹€æ…‹
        Env-->>Agent: (St+1, Rt, done)

        Agent->>Mem: å­˜å„²ç¶“é©— (St, At, Rt, St+1, done)

        alt è¨˜æ†¶é«”å……è¶³
            Agent->>Train: è§¸ç™¼è¨“ç·´
            Train->>Mem: éš¨æ©Ÿæ¡æ¨£ Batch
            Mem-->>Train: è¿”å› 64 ç­†ç¶“é©—
            Train->>DQN: è¨ˆç®—æå¤± L(Î¸)
            Train->>DQN: åå‘å‚³æ’­æ›´æ–° Î¸
            DQN-->>Train: è¿”å› Loss å€¼
        end

        alt Episode çµæŸ
            Train->>DQN: åŒæ­¥ Target Network
            Train-->>Dev: è¿”å›çµ±è¨ˆæ•¸æ“š
        end
    end
```

### 3.4 æ¨ç†æµç¨‹åºåˆ—åœ– (Inference MSC)

```mermaid
sequenceDiagram
    participant User as ç©å®¶
    participant UI as UI é¡¯ç¤º
    participant Env as éŠæˆ²å¼•æ“
    participant Agent as AI æ±ºç­–
    participant DQN as DQN æ¨¡å‹

    User->>UI: å•Ÿå‹•å°æˆ°æ¨¡å¼
    UI->>Agent: åŠ è¼‰è¨“ç·´å¥½çš„æ¨¡å‹
    Agent->>DQN: load_state_dict(Î¸*)
    DQN-->>Agent: æ¨¡å‹å°±ç·’

    UI->>Env: åˆå§‹åŒ–éŠæˆ²
    Env-->>UI: è¿”å›åˆå§‹ç‹€æ…‹ S0

    loop éŠæˆ²è¿´åœˆ
        UI->>Env: ç²å–ç•¶å‰ç‹€æ…‹ St
        Env-->>Agent: ç™¼é€ç‹€æ…‹ St

        Agent->>DQN: forward(St)
        DQN-->>Agent: Q å€¼ [Q1, Q2, Q3]
        Agent->>Agent: argmax(Q) â†’ é¸æ“‡æœ€ä½³å‹•ä½œ
        Agent-->>Env: åŸ·è¡Œå‹•ä½œ At

        Env->>Env: æ›´æ–°éŠæˆ²ç‹€æ…‹
        Env->>Env: æª¢æŸ¥å¾—åˆ†/ç¢°æ’
        Env-->>UI: æ¸²æŸ“ç•«é¢

        alt éŠæˆ²çµæŸ
            Env-->>UI: é¡¯ç¤ºæœ€çµ‚çµæœ
            UI-->>User: å±•ç¤ºçµ±è¨ˆæ•¸æ“š
        end
    end
```

### 3.5 æŠ€è¡“é¸å‹

| å±¤ç´š | æŠ€è¡“ | ç‰ˆæœ¬ | ç”¨é€” |
|:-----|:-----|:-----|:-----|
| **éŠæˆ²å¼•æ“** | Pygame / è‡ªå»º | 2.5+ | ç‰©ç†æ¨¡æ“¬èˆ‡æ¸²æŸ“ |
| **ML æ¡†æ¶** | PyTorch | 2.0+ | ç¥ç¶“ç¶²çµ¡å»ºæ§‹èˆ‡è¨“ç·´ |
| **RL ç®—æ³•** | Deep Q-Network | - | å¼·åŒ–å­¸ç¿’æ ¸å¿ƒç®—æ³• |
| **å„ªåŒ–å™¨** | Adam | - | åƒæ•¸æ›´æ–° |
| **GPU åŠ é€Ÿ** | CUDA | 11.8+ | è¨“ç·´åŠ é€Ÿ (NVIDIA) |
| **ç¨‹å¼èªè¨€** | Python | 3.8+ | ä¸»è¦é–‹ç™¼èªè¨€ |
| **æ•¸æ“šè™•ç†** | NumPy | 1.24+ | é™£åˆ—é‹ç®— |
| **è¦–è¦ºåŒ–** | Matplotlib | 3.7+ | è¨“ç·´æ›²ç·šç¹ªè£½ |

### 3.6 DQN ç¥ç¶“ç¶²çµ¡çµæ§‹ (å®Œæ•´ç‰ˆ)

#### 3.6.1 ç¶²çµ¡æ¶æ§‹åœ–

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    DQN ç¥ç¶“ç¶²çµ¡æ¶æ§‹                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

          è¼¸å…¥å±¤                  éš±è—å±¤ 1              éš±è—å±¤ 2
        (5å€‹ç¥ç¶“å…ƒ)            (64å€‹ç¥ç¶“å…ƒ)         (64å€‹ç¥ç¶“å…ƒ)
        
        â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                
        â”‚   ball_x   â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”         
        â”‚   ball_y   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  H1  â”‚ ReLUâ”œâ”€â”€â”€â”€â”€â”    
        â”‚   vel_x    â”‚         â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜     â”‚    
        â”‚   vel_y    â”‚                          â”Œâ”€â”€â”´â”€â”€â”€â”€â”
        â”‚  paddle_x  â”‚                          â”‚  H2   â”‚
        â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”Œâ”€â”€â”€â”€â”€â”¤ ReLU  â”œâ”€â”€â”€â”€â”€â”
                                          â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚


          éš±è—å±¤ 3              è¼¸å‡ºå±¤ (Q-å€¼)
        (32å€‹ç¥ç¶“å…ƒ)          (3å€‹å‹•ä½œ)
        
        â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”           
        â”‚  H3  â”‚ ReLU â”œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜      â”‚                    â”‚
                          â”Œâ”€â”€â–¼â”€â”€â”              â”Œâ”€â”€â–¼â”€â”€â”
                          â”‚ Q1  â”‚  Q(å·¦ç§»)     â”‚ Q2  â”‚
                          â””â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”˜
                                            â”Œâ”€â”€â”€â”€â”€â”€â”
                                            â”‚ Q3   â”‚ Q(å³ç§»)
                                            â””â”€â”€â”€â”€â”€â”€â”˜
                                         
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚ argmax(Q)    â”‚
                          â”‚ é¸æ“‡æœ€ä½³å‹•ä½œ â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### 3.6.2 åƒæ•¸è©³ç´°è¨ˆç®—è¡¨

| å±¤ç´š | å±¤é¡å‹ | è¼¸å…¥ç¶­åº¦ | è¼¸å‡ºç¶­åº¦ | æ¬Šé‡æ•¸ | åç½®æ•¸ | ç¸½åƒæ•¸æ•¸ | æ¿€æ´»å‡½æ•¸ |
|:-----|:-------|:----------|:----------|:--------|:--------|:-----------|:----------|
| Input | - | 5 | 5 | - | - | - | - |
| Layer 1 | Linear | 5 | 64 | 320 | 64 | **384** | ReLU |
| Layer 2 | Linear | 64 | 64 | 4,096 | 64 | **4,160** | ReLU |
| Layer 3 | Linear | 64 | 32 | 2,048 | 32 | **2,080** | ReLU |
| Output | Linear | 32 | 3 | 96 | 3 | **99** | Linear |
| | | | | | **ç¸½è¨ˆ** | **6,723** | |

#### 3.6.3 å‰å‘å‚³æ’­å…¬å¼

**ç¬¬1å±¤**: 
$$z_1 = W_1 \cdot x + b_1$$
$$a_1 = \text{ReLU}(z_1) = \max(0, z_1)$$

**ç¬¬2å±¤**: 
$$z_2 = W_2 \cdot a_1 + b_2$$
$$a_2 = \text{ReLU}(z_2)$$

**ç¬¬3å±¤**: 
$$z_3 = W_3 \cdot a_2 + b_3$$
$$a_3 = \text{ReLU}(z_3)$$

**è¼¸å‡ºå±¤**: 
$$Q_{values} = W_{out} \cdot a_3 + b_{out}$$

#### 3.6.4 ç¶²çµ¡åˆå§‹åŒ–ç­–ç•¥

```python
# æ¬Šé‡åˆå§‹åŒ–: Xavier å‡å‹»åˆ†ä½ˆ
W ~ Uniform[-âˆš(6/(n_in + n_out)), âˆš(6/(n_in + n_out))]

# åç½®åˆå§‹åŒ–: å…¨é›¶
b = 0

# Layer 1: W1 ~ Uniform[-âˆš(6/69), âˆš(6/69)]   # (5+64)
# Layer 2: W2 ~ Uniform[-âˆš(6/128), âˆš(6/128)] # (64+64)
# Layer 3: W3 ~ Uniform[-âˆš(6/96), âˆš(6/96)]   # (64+32)
# Output:  Wout ~ Uniform[-âˆš(6/35), âˆš(6/35)] # (32+3)
```

---

## ğŸ’» ç·¨ç¢¼å¯¦ç¾

### 4.1 æ ¸å¿ƒä»£ç¢¼çµæ§‹

```
pong_ai/
â”œâ”€â”€ main.py                 # ä¸»ç¨‹å¼å…¥å£
â”œâ”€â”€ game/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ engine.py          # éŠæˆ²å¼•æ“
â”‚   â”œâ”€â”€ physics.py         # ç‰©ç†æ¨¡æ“¬
â”‚   â””â”€â”€ renderer.py        # è¦–è¦ºåŒ–æ¸²æŸ“
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ dqn_model.py       # DQN ç¶²çµ¡å®šç¾©
â”‚   â”œâ”€â”€ agent.py           # Agent é‚è¼¯
â”‚   â””â”€â”€ replay_buffer.py   # ç¶“é©—å›æ”¾ç·©è¡
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ trainer.py         # è¨“ç·´ä¸»è¿´åœˆ
â”‚   â””â”€â”€ evaluator.py       # æ¨¡å‹è©•ä¼°
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ logger.py          # æ—¥èªŒè¨˜éŒ„
â”‚   â””â”€â”€ plotter.py         # çµæœç¹ªåœ–
â””â”€â”€ config.py              # é…ç½®åƒæ•¸
```

### 4.2 é—œéµå¯¦ç¾ç´°ç¯€

#### DQN æ¨¡å‹å®šç¾©

```python
import torch
import torch.nn as nn

class DQN(nn.Module):
    def __init__(self, state_dim=5, action_dim=3):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(state_dim, 64)
        self.fc2 = nn.Linear(64, 64)
        self.fc3 = nn.Linear(64, 32)
        self.out = nn.Linear(32, action_dim)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = torch.relu(self.fc3(x))
        return self.out(x)  # Linear output
```

#### Agent æ±ºç­–é‚è¼¯

```python
class Agent:
    def __init__(self, state_dim, action_dim):
        self.epsilon = 1.0
        self.epsilon_decay = 0.995
        self.epsilon_min = 0.05
        self.q_network = DQN(state_dim, action_dim)
        self.target_network = DQN(state_dim, action_dim)
        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=0.001)

    def select_action(self, state):
        if np.random.rand() < self.epsilon:
            return np.random.randint(0, 3)  # Explore
        else:
            with torch.no_grad():
                q_values = self.q_network(torch.FloatTensor(state))
                return torch.argmax(q_values).item()  # Exploit

    def update_epsilon(self):
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)
```

#### è¨“ç·´è¿´åœˆ

```python
def train_episode(env, agent, replay_buffer):
    state = env.reset()
    total_reward = 0
    done = False

    while not done:
        action = agent.select_action(state)
        next_state, reward, done, _ = env.step(action)

        replay_buffer.push(state, action, reward, next_state, done)

        if len(replay_buffer) > BATCH_SIZE:
            batch = replay_buffer.sample(BATCH_SIZE)
            loss = agent.train_step(batch)

        state = next_state
        total_reward += reward

    agent.update_epsilon()
    return total_reward
```

### 4.3 é…ç½®åƒæ•¸ (config.py)

```python
# éŠæˆ²åƒæ•¸
SCREEN_WIDTH = 800
SCREEN_HEIGHT = 600
FPS = 60
BALL_SPEED_RANGE = (5, 15)
PADDLE_SPEED = 10

# DQN åƒæ•¸
STATE_DIM = 5
ACTION_DIM = 3
HIDDEN_DIMS = [64, 64, 32]

# è¨“ç·´åƒæ•¸
LEARNING_RATE = 0.001
GAMMA = 0.99
EPSILON_START = 1.0
EPSILON_END = 0.05
EPSILON_DECAY = 0.995
BATCH_SIZE = 64
MEMORY_SIZE = 10000
TARGET_UPDATE_FREQ = 10

# è¨“ç·´è¨­ç½®
MAX_EPISODES = 1000
MAX_STEPS_PER_EPISODE = 1000
SAVE_INTERVAL = 50
```

---

## âœ… å¾…æ¸¬è©¦èˆ‡æ¸¬è©¦

### 5.1 æ¸¬è©¦è¨ˆåŠƒ

```mermaid
graph TB
    A[æ¸¬è©¦è¨ˆåŠƒ] --> B[å–®å…ƒæ¸¬è©¦]
    A --> C[é›†æˆæ¸¬è©¦]
    A --> D[æ€§èƒ½æ¸¬è©¦]
    A --> E[æ¨¡å‹æ¸¬è©¦]
    A --> F[ç©©å®šæ€§æ¸¬è©¦]

    B --> B1[çƒç‰©ç†æ¨¡æ“¬æ¸¬è©¦]
    B --> B2[ç¢°æ’åµæ¸¬æ¸¬è©¦]
    B --> B3[ç‹€æ…‹ç®¡ç†æ¸¬è©¦]
    B --> B4[ç¶²çµ¡å‰å‘å‚³æ’­æ¸¬è©¦]

    C --> C1[AI æ±ºç­–â†’éŠæˆ²åŸ·è¡Œ]
    C --> C2[è¨“ç·´æµç¨‹å®Œæ•´æ€§]
    C --> C3[æ¨¡å‹ä¿å­˜/åŠ è¼‰]

    D --> D1[æ¨ç†é€Ÿåº¦æ¸¬è©¦]
    D --> D2[è¨“ç·´æ•ˆç‡æ¸¬è©¦]
    D --> D3[è¨˜æ†¶é«”å ç”¨æ¸¬è©¦]

    E --> E1[æ¥çƒæˆåŠŸç‡æ¸¬è©¦]
    E --> E2[æ³›åŒ–èƒ½åŠ›æ¸¬è©¦]
    E --> E3[å°æŠ—æ¸¬è©¦]

    F --> F1[é•·æ™‚é–“é‹è¡Œæ¸¬è©¦]
    F --> F2[å¤šè¼ªè¨“ç·´ä¸€è‡´æ€§]
    F --> F3[é‚Šç•Œæ¢ä»¶æ¸¬è©¦]
```

### 5.2 æ¨¡å‹è©•ä¼°æŒ‡æ¨™

#### è¨“ç·´éç¨‹ç›£æ§

```python
# é—œéµæŒ‡æ¨™
metrics = {
    'episode_reward': [],      # æ¯å±€çå‹µ
    'moving_avg_reward': [],   # ç§»å‹•å¹³å‡çå‹µ (100 episodes)
    'loss': [],                # MSE Loss
    'epsilon': [],             # æ¢ç´¢ç‡
    'avg_q_value': [],         # å¹³å‡ Q å€¼
    'episode_length': []       # æ¯å±€æ­¥æ•¸
}
```

**è©•ä¼°å…¬å¼**:

1. **ç§»å‹•å¹³å‡çå‹µ**: 
$$R_{avg}(t) = \frac{1}{100}\sum_{i=t-99}^{t} R_i$$

2. **æå¤±å‡½æ•¸**: 
$$L(\theta) = \frac{1}{N}\sum_{i=1}^{N} (y_i - Q_\theta(s_i, a_i))^2$$

3. **æ¢ç´¢ç‡è¡°æ¸›**: 
$$\epsilon_t = \max(\epsilon_{min}, \epsilon_{start} \cdot \gamma_{\epsilon}^t)$$

4. **Q å€¼ä¼°è¨ˆ**: 
$$\bar{Q}(t) = \frac{1}{|B|}\sum_{(s,a) \in B} Q_\theta(s,a)$$

### 5.3 æ€§èƒ½åŸºæº–æ¸¬è©¦çµæœ

| æŒ‡æ¨™ | ç›®æ¨™å€¼ | å¯¦æ¸¬å€¼ | é”æˆç‡ | ç‹€æ…‹ |
|:-----|:-------|:-------|:-------|:-----|
| æ¥çƒæˆåŠŸç‡ | â‰¥85% | **å¾…æ¸¬è©¦** | - | â³ å¾…æ¸¬è©¦ |
| å¹³å‡éŠæˆ²æ™‚é•· | â‰¥100 çƒ | **å¾…æ¸¬è©¦** | - | â³ å¾…æ¸¬è©¦ |
| æ¨ç†å»¶é² | <30ms | **å¾…æ¸¬è©¦** | - | â³ å¾…æ¸¬è©¦ |
| è¨“ç·´æ™‚é–“ | â‰¤2å°æ™‚ | **å¾…æ¸¬è©¦** | - | â³ å¾…æ¸¬è©¦ |
| ç©©å®šæ€§ Ïƒ | <10% | **å¾…æ¸¬è©¦** | - | â³ å¾…æ¸¬è©¦ |
| è¨˜æ†¶é«”å ç”¨ | â‰¤2GB | **å¾…æ¸¬è©¦** | - | â³ å¾…æ¸¬è©¦ |

**æ¸¬è©¦ç’°å¢ƒ**:
- CPU: å¾…é…ç½®
- GPU: å¾…é…ç½®
- RAM: å¾…é…ç½®
- OS: Ubuntu 22.04 (ARM64)

### 5.4 è¨“ç·´æ›²ç·šåˆ†æ

**é æœŸè¨“ç·´æ›²ç·š**:

```
çå‹µå€¼ â†‘
  20 |                               â•±â”â”â”â”â”â”
     |                          â•±â”â”â”â”
  10 |                    â•±â”â”â”â”
     |              â•±â”â”â”â”
   0 |â”â”â”â”â”â”â”â•±â”â”â”â”
     |   â•±â”â”
 -10 |â”â”â”
     |
 -20 +â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â†’ Episodes
     0    100   200   300   400   500   600

æå¤±å€¼ â†“
 0.5 |â”â”â•²
     |    â•²â•²
 0.3 |      â•²â•²___
     |          â•²â•²___
 0.1 |              â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
     |
 0.0 +â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â†’ Episodes
     0    100   200   300   400   500   600

 0.5 |       â•²â•²â•²â•²___
     |             â•²â•²â•²___
 0.1 |                  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
     |
 0.0 +â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â†’ Episodes
     0    100   200   300   400   500   600
```

### 5.5 å¸¸è¦‹å•é¡Œæ’æŸ¥è¡¨

| å•é¡Œ | å¯èƒ½åŸå›  | è§£æ±ºæ–¹æ¡ˆ |
|:-----|:---------|:---------|
| è¨“ç·´ä¸æ”¶æ–‚ | å­¸ç¿’ç‡éå¤§ | é™ä½ LR è‡³ 0.0001 |
| | Replay Buffer å¤ªå° | å¢åŠ è‡³ 50000 |
| | Target æ›´æ–°å¤ªé »ç¹ | æ”¹ç‚ºæ¯ 100 episodes æ›´æ–° |
| éåº¦æ“¬åˆ | è¨“ç·´æ¨£æœ¬ä¸è¶³ | å¢åŠ æ¢ç´¢ç‡ Îµ |
| | ç¶²çµ¡å®¹é‡éå¤§ | æ¸›å°‘éš±è—å±¤ç¥ç¶“å…ƒæ•¸é‡ |
| æ¨ç†å»¶é²éé«˜ | GPU æœªå•Ÿç”¨ | æª¢æŸ¥ `torch.cuda.is_available()` |
| | Batch æ¨ç† | æ”¹ç‚ºå–®ç­†æ¨ç† |
| Loss éœ‡ç›ª | Batch size å¤ªå° | å¢åŠ è‡³ 128 |
| | æ²’ç”¨ Target Network | ç¢ºèª Target ç¶²çµ¡æ­£ç¢ºæ›´æ–° |
| Q å€¼çˆ†ç‚¸ | Reward æœªæ¨™æº–åŒ– | å°‡ reward é™åˆ¶åœ¨ [-1, 1] |
| | Gamma å€¼éå¤§ | é™ä½è‡³ 0.95 |

### 5.6 åŠŸèƒ½é©—æ”¶æª¢æŸ¥è¡¨

| ID | åŠŸèƒ½é …ç›® | æ¸¬è©¦æ–¹æ³• | é€šéæ¨™æº– | ç‹€æ…‹ |
|:---|:---------|:---------|:---------|:-----|
| F1 | éŠæˆ²ç’°å¢ƒåˆå§‹åŒ– | å–®å…ƒæ¸¬è©¦ | ç„¡éŒ¯èª¤å•Ÿå‹• | â³ å¾…æ¸¬è©¦ |
| F2 | çƒç‰©ç†é‹å‹• | è¦–è¦ºæª¢æŸ¥ | è»Œè·¡åˆç† | â³ å¾…æ¸¬è©¦ |
| F3 | çƒæ‹æ§åˆ¶ | æ‰‹å‹•æ¸¬è©¦ | éŸ¿æ‡‰ <50ms | â³ å¾…æ¸¬è©¦ |
| F4 | AI æ±ºç­– | æ¨ç†æ¸¬è©¦ | å»¶é² <30ms | â³ å¾…æ¸¬è©¦ |
| F5 | å¾—åˆ†è¨ˆç®— | å–®å…ƒæ¸¬è©¦ | è¨ˆåˆ†æ­£ç¢º | â³ å¾…æ¸¬è©¦ |
| F6 | æ¨¡å¼åˆ‡æ› | é›†æˆæ¸¬è©¦ | ç„¡éŒ¯èª¤åˆ‡æ› | â³ å¾…æ¸¬è©¦ |
| F7 | æ¨¡å‹ä¿å­˜/åŠ è¼‰ | æ–‡ä»¶æ¸¬è©¦ | å®Œæ•´é‚„åŸ | â³ å¾…æ¸¬è©¦ |
| F8 | è¦–è¦ºåŒ–å‘ˆç¾ | UI æ¸¬è©¦ | æ¸…æ™°å¯è®€ | â³ å¾…æ¸¬è©¦ |

### 5.7 å£“åŠ›æ¸¬è©¦

**æ¸¬è©¦å ´æ™¯**:
- é€£çºŒé‹è¡Œ 10000 episodes
- è¨˜æ†¶é«”æ´©æ¼æª¢æ¸¬
- GPU åˆ©ç”¨ç‡ç›£æ§

**çµæœ**:
```
â³ å¾…æ¸¬è©¦
```

---

## ğŸ“š åƒè€ƒè³‡æº

### è«–æ–‡èˆ‡æ–‡ç»

1. **Mnih et al. (2015)** - [Human-level control through deep reinforcement learning](https://www.nature.com/articles/nature14236)  
   *Nature, 518(7540), 529-533*

2. **Van Hasselt et al. (2016)** - [Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/abs/1509.06461)  
   *AAAI Conference on Artificial Intelligence*

3. **Schaul et al. (2016)** - [Prioritized Experience Replay](https://arxiv.org/abs/1511.05952)  
   *ICLR 2016*

4. **Wang et al. (2016)** - [Dueling Network Architectures for Deep Reinforcement Learning](https://arxiv.org/abs/1511.06581)  
   *ICML 2016*

### é–‹æºæ¡†æ¶

| æ¡†æ¶ | éˆæ¥ | ç”¨é€” |
|:-----|:-----|:-----|
| PyTorch | [pytorch.org](https://pytorch.org) | æ·±åº¦å­¸ç¿’æ¡†æ¶ |
| OpenAI Gym | [gym.openai.com](https://gym.openai.com) | RL ç’°å¢ƒæ¨™æº– |
| Stable Baselines3 | [stable-baselines3.readthedocs.io](https://stable-baselines3.readthedocs.io) | RL ç®—æ³•åº« |
| Pygame | [pygame.org](https://www.pygame.org) | éŠæˆ²é–‹ç™¼æ¡†æ¶ |

### ç›¸é—œå°ˆæ¡ˆ

- [TetrAI](https://github.com/takado8/tetris_ai_deep_reinforcement_learning) - ä¿„ç¾…æ–¯æ–¹å¡Š DQN
- [OpenAI Five](https://openai.com/research/openai-five) - Dota 2 å¤šæ™ºèƒ½é«”ç³»çµ±
- [AlphaGo/AlphaZero](https://deepmind.google/technologies/alphago/) - åœæ£‹ AI é‡Œç¨‹ç¢‘

---

## ğŸ“Š é …ç›®ç‹€æ…‹

```
å®Œæˆåº¦:
â”œâ”€ éœ€æ±‚åˆ†æ âœ… 100%
â”œâ”€ ç³»çµ±åˆ†æ âœ… 100%
â”œâ”€ ç³»çµ±è¨­è¨ˆ âœ… 100%
â”œâ”€ ç·¨ç¢¼å¯¦ç¾ ğŸ”„ 80% (å¾…å®Œå–„æ–‡æª”)
â””â”€ å¾…æ¸¬è©¦æ¸¬è©¦ â³ 0% (å°šæœªé–‹å§‹)
```

**æœ€å¾Œæ›´æ–°**: 2025-12-04  
**ç‰ˆæœ¬**: v1.0.1  
**æˆæ¬Š**: MIT License
