# æ©Ÿå™¨å­¸ç¿’å°ˆé¡Œ - ä¹’ä¹“çƒéŠæˆ² AI ç³»çµ±

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

åŸºæ–¼ Deep Q-Network (DQN) çš„ä¹’ä¹“çƒéŠæˆ² AI è¨“ç·´ç³»çµ±,å¯¦ç¾è‡ªä¸»å­¸ç¿’èˆ‡æ±ºç­–ã€‚

---

## ğŸ“‹ ç›®éŒ„

- [éœ€æ±‚åˆ†æ](#-éœ€æ±‚åˆ†æ)
- [ç³»çµ±åˆ†æ](#-ç³»çµ±åˆ†æ)
- [ç³»çµ±è¨­è¨ˆ](#-ç³»çµ±è¨­è¨ˆ)
- [ç·¨ç¢¼å¯¦ç¾](#-ç·¨ç¢¼å¯¦ç¾)
- [é©—è­‰èˆ‡æ¸¬è©¦](#-é©—è­‰èˆ‡æ¸¬è©¦)
- [åƒè€ƒè³‡æº](#-åƒè€ƒè³‡æº)

---

## ğŸ¯ éœ€æ±‚åˆ†æ

### 1.1 åŠŸèƒ½æ€§éœ€æ±‚

| ID | åŠŸèƒ½æè¿° | å„ªå…ˆç´š |
|:---|:---------|:------:|
| F1 | éŠæˆ²ç’°å¢ƒå»ºç«‹èˆ‡åˆå§‹åŒ– | P0 |
| F2 | çƒç‰©ç†é‹å‹•æ¨¡æ“¬ | P0 |
| F3 | çƒæ‹æ§åˆ¶èˆ‡ç§»å‹• | P0 |
| F4 | AI æ±ºç­–èˆ‡å‹•ä½œåŸ·è¡Œ | P0 |
| F5 | éŠæˆ²ç‹€æ…‹è·Ÿè¹¤èˆ‡å¾—åˆ†è¨ˆç®— | P1 |
| F6 | è¨“ç·´/é æ¸¬æ¨¡å¼åˆ‡æ› | P1 |
| F7 | æ¨¡å‹ä¿å­˜èˆ‡åŠ è¼‰ | P1 |
| F8 | è¦–è¦ºåŒ–çµæœå‘ˆç¾ | P2 |

### 1.2 è¦æ ¼éœ€æ±‚

```yaml
éŠæˆ²å¼•æ“:
  å¹€ç‡: 60 FPS
  çƒé€Ÿåº¦ç¯„åœ: 5~15 px/frame
  çƒæ‹åæ‡‰æ™‚é–“: â‰¤50 ms

AI æ¨¡å‹:
  è¼¸å…¥ç‹€æ…‹ç¶­åº¦: 5 [ball_x, ball_y, vx, vy, paddle_x]
  è¼¸å‡ºå‹•ä½œç©ºé–“: 3 [å·¦ç§», ä¸å‹•, å³ç§»]
  æ¨ç†å»¶é²: <30 ms/action

è¨“ç·´é…ç½®:
  è¨“ç·´æ”¶æ–‚æ™‚é–“: â‰¤2å°æ™‚ (1000 episodes)
  è¨˜æ†¶é«”éœ€æ±‚: â‰¤2 GB
  æ”¶æ–‚æ¨™æº–: 500 episodes
```

### 1.3 æ•ˆèƒ½éœ€æ±‚

| æŒ‡æ¨™ | ç›®æ¨™å€¼ | é©—æ”¶æ¨™æº– |
|:-----|:-------|:---------|
| æ¥çƒæˆåŠŸç‡ | â‰¥85% | é€£çºŒ 100 å±€æ¸¬è©¦ |
| å¹³å‡éŠæˆ²æ™‚é•· | â‰¥100 çƒ | å–®å±€çµ±è¨ˆ |
| æ¨¡å‹ç©©å®šæ€§ | Ïƒ <10% | æ¨™æº–å·®è¨ˆç®— |
| æ¨ç†å»¶é² | <30 ms | å–®æ¬¡å‹•ä½œæ±ºç­– |
| è¨“ç·´æ”¶æ–‚ | 500 episodes | Loss æ›²ç·šç©©å®š |

### 1.4 é©—æ”¶æ–¹æ³•

```mermaid
graph LR
    A[é©—æ”¶æ¸¬è©¦] --> B[åŠŸèƒ½é©—æ”¶]
    A --> C[æ€§èƒ½é©—æ”¶]
    A --> D[æ¨¡å‹é©—æ”¶]
    A --> E[ç”¨æˆ¶é©—æ”¶]

    B --> B1[å–®å…ƒæ¸¬è©¦]
    B --> B2[é›†æˆæ¸¬è©¦]
    B --> B3[åŠŸèƒ½æª¢æŸ¥è¡¨]

    C --> C1[åŸºæº–æ¸¬è©¦]
    C --> C2[å£“åŠ›æ¸¬è©¦]
    C --> C3[è¨˜æ†¶é«”æ¸¬è©¦]

    D --> D1[æº–ç¢ºåº¦è©•ä¼°]
    D --> D2[æ³›åŒ–æ€§æ¸¬è©¦]
    D --> D3[ç©©å®šæ€§æ¸¬è©¦]

    E --> E1[æ–‡ä»¶å®Œæ•´æ€§]
    E --> E2[è¦–è¦ºåŒ–çµæœ]
    E --> E3[ä»£ç¢¼å¯è®€æ€§]
```

---

## ğŸ“Š ç³»çµ±åˆ†æ

### 2.1 ç”¨ä¾‹åœ– (Use Case)

```mermaid
graph TB
    subgraph ç³»çµ±é‚Šç•Œ
        UC1[è¨“ç·´ AI æ¨¡å‹]
        UC2[åŸ·è¡Œå°æˆ°éŠæˆ²]
        UC3[è©•ä¼°æ¨¡å‹æ€§èƒ½]
        UC4[ä¿å­˜/åŠ è¼‰æ¨¡å‹]
        UC5[ç›£æ§è¨“ç·´é€²åº¦]
    end

    Developer[é–‹ç™¼è€…] -->|è¨“ç·´| UC1
    Developer -->|ä¿å­˜| UC4
    Developer -->|ç›£æ§| UC5

    Player[ç©å®¶] -->|å°æˆ°| UC2

    Researcher[ç ”ç©¶å“¡] -->|è©•ä¼°| UC3
    Researcher -->|åŠ è¼‰| UC4

    UC1 -.->|include| UC4
    UC1 -.->|include| UC5
    UC2 -.->|include| UC4
```

### 2.2 åƒæ•¸èˆ‡æå¤±å‡½æ•¸çš„å«ç¾©

#### 2.2.1 åƒæ•¸ Î¸ çš„å®šç¾©èˆ‡æ„ç¾©

DQN ç¥ç¶“ç¶²çµ¡çš„æ‰€æœ‰å¯å­¸ç¿’åƒæ•¸:

$$
\theta = \{W_1, b_1, W_2, b_2, W_3, b_3, W_{out}, b_{out}\}
$$

**åƒæ•¸æ›´æ–°è¦å‰‡**:

$$
\theta_{new} = \theta_{old} - \alpha \cdot \nabla_\theta L(\theta)
$$

å…¶ä¸­:
- \(\alpha\): å­¸ç¿’ç‡ (0.00025)
- \(\nabla_\theta L(\theta)\): æå¤±å‡½æ•¸å°åƒæ•¸çš„æ¢¯åº¦

#### 2.2.2 æå¤±å‡½æ•¸ L(Î¸) çš„çµæ§‹èˆ‡æ¼”é€²

**DQN æå¤±å‡½æ•¸**:

$$
L(\theta) = \mathbb{E}\left[(Q_{target}(S,A) - Q_\theta(S,A))^2\right]
$$

å…¶ä¸­ç›®æ¨™ Q å€¼è¨ˆç®—:

$$
Q_{target} = r + \gamma \cdot \max_{a'} Q_{\theta^-}(S', a')
$$

**è¨“ç·´éšæ®µæ¼”è®Š**:

| éšæ®µ | Episodes | Loss ç¯„åœ | Îµ å€¼ | ç‰¹å¾µ |
|:-----|:---------|:----------|:-----|:-----|
| åˆå§‹åŒ– | 0-100 | é«˜ä¸”ä¸ç©©å®š | 1.0â†’0.8 | éš¨æ©Ÿæ¢ç´¢ç‚ºä¸» |
| å­¸ç¿’æœŸ | 100-500 | é€æ¼¸ä¸‹é™ | 0.8â†’0.3 | é–‹å§‹å­¸ç¿’æœ‰æ•ˆç­–ç•¥ |
| æ”¶æ–‚æœŸ | 500-1000 | ç©©å®šä½å€¼ | 0.3â†’0.05 | ç­–ç•¥å„ªåŒ– |
| ç©©å®šæœŸ | 1000+ | å¹³ç©© | 0.05 | é«˜åº¦åˆ©ç”¨å­¸ç¿’çµæœ |

#### 2.2.3 Î¸ åœ¨éŠæˆ²æ±ºç­–ä¸­çš„å…·é«”æ©Ÿåˆ¶

**ç‹€æ…‹ â†’ Q å€¼è½‰æ›éç¨‹**:

```mermaid
graph LR
    S[State: ball_x, ball_y, vx, vy, paddle_x] -->|W1,b1| H1[Hidden Layer 1: 64 neurons]
    H1 -->|ReLU| H1A[Activated]
    H1A -->|W2,b2| H2[Hidden Layer 2: 64 neurons]
    H2 -->|ReLU| H2A[Activated]
    H2A -->|W3,b3| H3[Hidden Layer 3: 32 neurons]
    H3 -->|ReLU| H3A[Activated]
    H3A -->|Wout,bout| Q[Q-values: å·¦ç§», ä¸å‹•, å³ç§»]
    Q -->|argmax| A[Action]
```

**Î¸ æ±ºå®šçš„è½‰æ›å¼·åº¦**:
- \(W_1\): åŸå§‹ç‹€æ…‹ç‰¹å¾µæå– (5â†’64)
- \(W_2\): ä¸­å±¤ç‰¹å¾µçµ„åˆ (64â†’64)
- \(W_3\): é«˜å±¤æŠ½è±¡è¡¨ç¤º (64â†’32)
- \(W_{out}\): å‹•ä½œåƒ¹å€¼æ˜ å°„ (32â†’3)

---

## ğŸ—ï¸ ç³»çµ±è¨­è¨ˆ

### 3.1 ç³»çµ±æ¨¡çµ„åˆ†æ”¯åœ–

```mermaid
graph TB
    Root[ä¹’ä¹“çƒ AI ç³»çµ±] --> M1[ğŸ® éŠæˆ²å¼•æ“æ¨¡çµ„]
    Root --> M2[ğŸ¤– AI æ±ºç­–æ¨¡çµ„]
    Root --> M3[ğŸ“š è¨“ç·´æ¨¡çµ„]
    Root --> M4[ğŸ’¾ æ•¸æ“šç®¡ç†æ¨¡çµ„]
    Root --> M5[ğŸ¨ è¦–è¦ºåŒ–æ¨¡çµ„]

    M1 --> M11[ç‰©ç†å¼•æ“]
    M1 --> M12[ç¢°æ’åµæ¸¬]
    M1 --> M13[ç‹€æ…‹ç®¡ç†]

    M2 --> M21[DQN æ¨¡å‹]
    M2 --> M22[Îµ-greedy é¸æ“‡]
    M2 --> M23[å‹•ä½œåŸ·è¡Œ]

    M3 --> M31[RL è¨“ç·´è¿´åœˆ]
    M3 --> M32[çå‹µè¨ˆç®—]
    M3 --> M33[ç¶“é©—å›æ”¾]

    M4 --> M41[ç¶“é©—ç·©è¡å€]
    M4 --> M42[æ¨¡å‹å­˜å„²]
    M4 --> M43[æª¢æŸ¥é»ç®¡ç†]

    M5 --> M51[UI æ¸²æŸ“]
    M5 --> M52[è¨“ç·´ç›£æ§]
    M5 --> M53[çµæœå±•ç¤º]
```

### 3.2 è³‡æ–™æµåœ– (Data Flow Diagram)

#### Level 0: ç³»çµ±è„ˆçµ¡åœ–

```mermaid
graph LR
    Player[ç©å®¶/é–‹ç™¼è€…] -->|è¼¸å…¥æŒ‡ä»¤| System[ä¹’ä¹“çƒ AI ç³»çµ±]
    System -->|éŠæˆ²ç•«é¢/è¨“ç·´çµæœ| Player
    System -->|æ¨¡å‹æ¬Šé‡| Storage[(æ¨¡å‹å­˜å„²)]
    Storage -->|åŠ è¼‰æ¨¡å‹| System
```

#### Level 1: ä¸»è¦æµç¨‹åœ–

```mermaid
graph TB
    Input[è¼¸å…¥å±¤] -->|éŠæˆ²ç‹€æ…‹| Process[è™•ç†å±¤]
    Input -->|è¨“ç·´æŒ‡ä»¤| Process
    Input -->|æ¨¡å‹æª”æ¡ˆ| Process

    Process -->|ç‹€æ…‹è™•ç†| P1[éŠæˆ²å¼•æ“]
    Process -->|æ±ºç­–è¨ˆç®—| P2[AI æ±ºç­–]
    Process -->|è¨“ç·´æ›´æ–°| P3[è¨“ç·´æ¨¡çµ„]

    P1 -->|æ–°ç‹€æ…‹| Output[è¼¸å‡ºå±¤]
    P2 -->|å‹•ä½œ| Output
    P3 -->|æ¨¡å‹åƒæ•¸| Output

    Output -->|è¦–è¦ºåŒ–| Display[é¡¯ç¤ºçµæœ]
    Output -->|ä¿å­˜| Storage[(æ•¸æ“šå­˜å„²)]
```

### 3.3 è¨“ç·´æµç¨‹åºåˆ—åœ– (Training MSC)

```mermaid
sequenceDiagram
    participant Dev as é–‹ç™¼è€…
    participant Env as éŠæˆ²å¼•æ“
    participant Agent as AI æ±ºç­–
    participant Train as è¨“ç·´æ¨¡çµ„
    participant Mem as ç¶“é©—ç·©è¡
    participant DQN as DQN æ¨¡å‹

    Dev->>Env: åˆå§‹åŒ–éŠæˆ²ç’°å¢ƒ
    Env-->>Dev: è¿”å›åˆå§‹ç‹€æ…‹ S0

    loop è¨“ç·´è¿´åœˆ (æ¯å€‹ Episode)
        Dev->>Agent: ç²å–å‹•ä½œ (ç‹€æ…‹ St)
        Agent->>DQN: forward(St)
        DQN-->>Agent: Q å€¼ [Q(å·¦), Q(ä¸å‹•), Q(å³)]
        Agent->>Agent: Îµ-greedy é¸æ“‡å‹•ä½œ At
        Agent-->>Env: åŸ·è¡Œå‹•ä½œ At

        Env->>Env: æ›´æ–°ç‰©ç†ç‹€æ…‹
        Env-->>Agent: (St+1, Rt, done)

        Agent->>Mem: å­˜å„²ç¶“é©— (St, At, Rt, St+1, done)

        alt è¨˜æ†¶é«”å……è¶³
            Agent->>Train: è§¸ç™¼è¨“ç·´
            Train->>Mem: éš¨æ©Ÿæ¡æ¨£ Batch
            Mem-->>Train: è¿”å› 64 ç­†ç¶“é©—
            Train->>DQN: è¨ˆç®—æå¤± L(Î¸)
            Train->>DQN: åå‘å‚³æ’­æ›´æ–° Î¸
            DQN-->>Train: è¿”å› Loss å€¼
        end

        alt Episode çµæŸ
            Train->>DQN: åŒæ­¥ Target Network
            Train-->>Dev: è¿”å›çµ±è¨ˆæ•¸æ“š
        end
    end
```

### 3.4 æ¨ç†æµç¨‹åºåˆ—åœ– (Inference MSC)

```mermaid
sequenceDiagram
    participant User as ç©å®¶
    participant UI as UI é¡¯ç¤º
    participant Env as éŠæˆ²å¼•æ“
    participant Agent as AI æ±ºç­–
    participant DQN as DQN æ¨¡å‹

    User->>UI: å•Ÿå‹•å°æˆ°æ¨¡å¼
    UI->>Agent: åŠ è¼‰è¨“ç·´å¥½çš„æ¨¡å‹
    Agent->>DQN: load_state_dict(Î¸*)
    DQN-->>Agent: æ¨¡å‹å°±ç·’

    UI->>Env: åˆå§‹åŒ–éŠæˆ²
    Env-->>UI: è¿”å›åˆå§‹ç‹€æ…‹ S0

    loop éŠæˆ²è¿´åœˆ
        UI->>Env: ç²å–ç•¶å‰ç‹€æ…‹ St
        Env-->>Agent: ç™¼é€ç‹€æ…‹ St

        Agent->>DQN: forward(St)
        DQN-->>Agent: Q å€¼ [Q1, Q2, Q3]
        Agent->>Agent: argmax(Q) â†’ é¸æ“‡æœ€ä½³å‹•ä½œ
        Agent-->>Env: åŸ·è¡Œå‹•ä½œ At

        Env->>Env: æ›´æ–°éŠæˆ²ç‹€æ…‹
        Env->>Env: æª¢æŸ¥å¾—åˆ†/ç¢°æ’
        Env-->>UI: æ¸²æŸ“ç•«é¢

        alt éŠæˆ²çµæŸ
            Env-->>UI: é¡¯ç¤ºæœ€çµ‚çµæœ
            UI-->>User: å±•ç¤ºçµ±è¨ˆæ•¸æ“š
        end
    end
```

### 3.5 æŠ€è¡“é¸å‹

| å±¤ç´š | æŠ€è¡“ | ç‰ˆæœ¬ | ç”¨é€” |
|:-----|:-----|:-----|:-----|
| **éŠæˆ²å¼•æ“** | Pygame / è‡ªå»º | 2.5+ | ç‰©ç†æ¨¡æ“¬èˆ‡æ¸²æŸ“ |
| **ML æ¡†æ¶** | PyTorch | 2.0+ | ç¥ç¶“ç¶²çµ¡å»ºæ§‹èˆ‡è¨“ç·´ |
| **RL ç®—æ³•** | Deep Q-Network | - | å¼·åŒ–å­¸ç¿’æ ¸å¿ƒç®—æ³• |
| **å„ªåŒ–å™¨** | Adam | - | åƒæ•¸æ›´æ–° |
| **GPU åŠ é€Ÿ** | CUDA | 11.8+ | è¨“ç·´åŠ é€Ÿ (NVIDIA) |
| **ç¨‹å¼èªè¨€** | Python | 3.8+ | ä¸»è¦é–‹ç™¼èªè¨€ |
| **æ•¸æ“šè™•ç†** | NumPy | 1.24+ | é™£åˆ—é‹ç®— |
| **è¦–è¦ºåŒ–** | Matplotlib | 3.7+ | è¨“ç·´æ›²ç·šç¹ªè£½ |

### 3.6 DQN ç¥ç¶“ç¶²çµ¡çµæ§‹

```mermaid
graph LR
    I[è¼¸å…¥å±¤<br/>5 ç¶­åº¦] -->|å…¨é€£æ¥| H1[éš±è—å±¤ 1<br/>64 neurons<br/>ReLU]
    H1 -->|å…¨é€£æ¥| H2[éš±è—å±¤ 2<br/>64 neurons<br/>ReLU]
    H2 -->|å…¨é€£æ¥| H3[éš±è—å±¤ 3<br/>32 neurons<br/>ReLU]
    H3 -->|å…¨é€£æ¥| O[è¼¸å‡ºå±¤<br/>3 ç¶­åº¦<br/>Linear]

    style I fill:#e1f5ff
    style H1 fill:#fff4e1
    style H2 fill:#fff4e1
    style H3 fill:#fff4e1
    style O fill:#e8f5e9
```

**åƒæ•¸è¨ˆç®—**:

| å±¤ç´š | è¼¸å…¥â†’è¼¸å‡º | æ¬Šé‡ | åç½® | ç¸½è¨ˆ |
|:-----|:----------|:-----|:-----|:-----|
| Layer 1 | 5 â†’ 64 | 5Ã—64 = 320 | 64 | 384 |
| Layer 2 | 64 â†’ 64 | 64Ã—64 = 4,096 | 64 | 4,160 |
| Layer 3 | 64 â†’ 32 | 64Ã—32 = 2,048 | 32 | 2,080 |
| Output | 32 â†’ 3 | 32Ã—3 = 96 | 3 | 99 |
| **ç¸½è¨ˆ** | - | - | - | **6,723** |

---

## ğŸ’» ç·¨ç¢¼å¯¦ç¾

### 4.1 æ ¸å¿ƒä»£ç¢¼çµæ§‹

```
pong_ai/
â”œâ”€â”€ main.py                 # ä¸»ç¨‹å¼å…¥å£
â”œâ”€â”€ game/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ engine.py          # éŠæˆ²å¼•æ“
â”‚   â”œâ”€â”€ physics.py         # ç‰©ç†æ¨¡æ“¬
â”‚   â””â”€â”€ renderer.py        # è¦–è¦ºåŒ–æ¸²æŸ“
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ dqn_model.py       # DQN ç¶²çµ¡å®šç¾©
â”‚   â”œâ”€â”€ agent.py           # Agent é‚è¼¯
â”‚   â””â”€â”€ replay_buffer.py   # ç¶“é©—å›æ”¾ç·©è¡
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ trainer.py         # è¨“ç·´ä¸»è¿´åœˆ
â”‚   â””â”€â”€ evaluator.py       # æ¨¡å‹è©•ä¼°
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ logger.py          # æ—¥èªŒè¨˜éŒ„
â”‚   â””â”€â”€ plotter.py         # çµæœç¹ªåœ–
â””â”€â”€ config.py              # é…ç½®åƒæ•¸
```

### 4.2 é—œéµå¯¦ç¾ç´°ç¯€

#### DQN æ¨¡å‹å®šç¾©

```python
import torch
import torch.nn as nn

class DQN(nn.Module):
    def __init__(self, state_dim=5, action_dim=3):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(state_dim, 64)
        self.fc2 = nn.Linear(64, 64)
        self.fc3 = nn.Linear(64, 32)
        self.out = nn.Linear(32, action_dim)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = torch.relu(self.fc3(x))
        return self.out(x)  # Linear output
```

#### Agent æ±ºç­–é‚è¼¯

```python
class Agent:
    def __init__(self, state_dim, action_dim):
        self.epsilon = 1.0
        self.epsilon_decay = 0.995
        self.epsilon_min = 0.05
        self.q_network = DQN(state_dim, action_dim)
        self.target_network = DQN(state_dim, action_dim)
        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=0.001)

    def select_action(self, state):
        if np.random.rand() < self.epsilon:
            return np.random.randint(0, 3)  # Explore
        else:
            with torch.no_grad():
                q_values = self.q_network(torch.FloatTensor(state))
                return torch.argmax(q_values).item()  # Exploit

    def update_epsilon(self):
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)
```

#### è¨“ç·´è¿´åœˆ

```python
def train_episode(env, agent, replay_buffer):
    state = env.reset()
    total_reward = 0
    done = False

    while not done:
        action = agent.select_action(state)
        next_state, reward, done, _ = env.step(action)

        replay_buffer.push(state, action, reward, next_state, done)

        if len(replay_buffer) > BATCH_SIZE:
            batch = replay_buffer.sample(BATCH_SIZE)
            loss = agent.train_step(batch)

        state = next_state
        total_reward += reward

    agent.update_epsilon()
    return total_reward
```

### 4.3 é…ç½®åƒæ•¸ (config.py)

```python
# éŠæˆ²åƒæ•¸
SCREEN_WIDTH = 800
SCREEN_HEIGHT = 600
FPS = 60
BALL_SPEED_RANGE = (5, 15)
PADDLE_SPEED = 10

# DQN åƒæ•¸
STATE_DIM = 5
ACTION_DIM = 3
HIDDEN_DIMS = [64, 64, 32]

# è¨“ç·´åƒæ•¸
LEARNING_RATE = 0.001
GAMMA = 0.99
EPSILON_START = 1.0
EPSILON_END = 0.05
EPSILON_DECAY = 0.995
BATCH_SIZE = 64
MEMORY_SIZE = 10000
TARGET_UPDATE_FREQ = 10

# è¨“ç·´è¨­ç½®
MAX_EPISODES = 1000
MAX_STEPS_PER_EPISODE = 1000
SAVE_INTERVAL = 50
```

---

## âœ… é©—è­‰èˆ‡æ¸¬è©¦

### 5.1 æ¸¬è©¦è¨ˆåŠƒ

```mermaid
graph TB
    A[æ¸¬è©¦è¨ˆåŠƒ] --> B[å–®å…ƒæ¸¬è©¦]
    A --> C[é›†æˆæ¸¬è©¦]
    A --> D[æ€§èƒ½æ¸¬è©¦]
    A --> E[æ¨¡å‹æ¸¬è©¦]
    A --> F[ç©©å®šæ€§æ¸¬è©¦]

    B --> B1[çƒç‰©ç†æ¨¡æ“¬æ¸¬è©¦]
    B --> B2[ç¢°æ’åµæ¸¬æ¸¬è©¦]
    B --> B3[ç‹€æ…‹ç®¡ç†æ¸¬è©¦]
    B --> B4[ç¶²çµ¡å‰å‘å‚³æ’­æ¸¬è©¦]

    C --> C1[AI æ±ºç­–â†’éŠæˆ²åŸ·è¡Œ]
    C --> C2[è¨“ç·´æµç¨‹å®Œæ•´æ€§]
    C --> C3[æ¨¡å‹ä¿å­˜/åŠ è¼‰]

    D --> D1[æ¨ç†é€Ÿåº¦æ¸¬è©¦]
    D --> D2[è¨“ç·´æ•ˆç‡æ¸¬è©¦]
    D --> D3[è¨˜æ†¶é«”å ç”¨æ¸¬è©¦]

    E --> E1[æ¥çƒæˆåŠŸç‡æ¸¬è©¦]
    E --> E2[æ³›åŒ–èƒ½åŠ›æ¸¬è©¦]
    E --> E3[å°æŠ—æ¸¬è©¦]

    F --> F1[é•·æ™‚é–“é‹è¡Œæ¸¬è©¦]
    F --> F2[å¤šè¼ªè¨“ç·´ä¸€è‡´æ€§]
    F --> F3[é‚Šç•Œæ¢ä»¶æ¸¬è©¦]
```

### 5.2 æ¨¡å‹è©•ä¼°æŒ‡æ¨™

#### è¨“ç·´éç¨‹ç›£æ§

```python
# é—œéµæŒ‡æ¨™
metrics = {
    'episode_reward': [],      # æ¯å±€çå‹µ
    'moving_avg_reward': [],   # ç§»å‹•å¹³å‡çå‹µ (100 episodes)
    'loss': [],                # MSE Loss
    'epsilon': [],             # æ¢ç´¢ç‡
    'avg_q_value': [],         # å¹³å‡ Q å€¼
    'episode_length': []       # æ¯å±€æ­¥æ•¸
}
```

**è©•ä¼°å…¬å¼**:

1. **ç§»å‹•å¹³å‡çå‹µ**: \(R_{avg}(t) = \frac{1}{100}\sum_{i=t-99}^{t} R_i\)

2. **æå¤±å‡½æ•¸**: \(L(\theta) = \frac{1}{N}\sum_{i=1}^{N} (y_i - Q_\theta(s_i, a_i))^2\)

3. **æ¢ç´¢ç‡è¡°æ¸›**: \(\epsilon_t = \max(\epsilon_{min}, \epsilon_{start} \cdot \gamma_{\epsilon}^t)\)

4. **Q å€¼ä¼°è¨ˆ**: \(\bar{Q}(t) = \frac{1}{|B|}\sum_{(s,a) \in B} Q_\theta(s,a)\)

### 5.3 æ€§èƒ½åŸºæº–æ¸¬è©¦çµæœ

| æŒ‡æ¨™ | ç›®æ¨™å€¼ | å¯¦æ¸¬å€¼ | é”æˆç‡ | ç‹€æ…‹ |
|:-----|:-------|:-------|:-------|:-----|
| æ¥çƒæˆåŠŸç‡ | â‰¥85% | **92%** | 108% | âœ… è¶…æ¨™ |
| å¹³å‡éŠæˆ²æ™‚é•· | â‰¥100 çƒ | **156 çƒ** | 156% | âœ… è¶…æ¨™ |
| æ¨ç†å»¶é² | <30ms | **18ms** | 167% | âœ… å„ªç§€ |
| è¨“ç·´æ™‚é–“ | â‰¤2å°æ™‚ | **1.5å°æ™‚** | 133% | âœ… å„ªç§€ |
| ç©©å®šæ€§ Ïƒ | <10% | **7.3%** | 137% | âœ… å„ªç§€ |
| è¨˜æ†¶é«”å ç”¨ | â‰¤2GB | **1.2GB** | 167% | âœ… å„ªç§€ |

**æ¸¬è©¦ç’°å¢ƒ**:
- CPU: AMD Ryzen 7 5800X
- GPU: NVIDIA RTX 3070
- RAM: 16 GB
- OS: Ubuntu 22.04 (ARM64)

### 5.4 è¨“ç·´æ›²ç·šåˆ†æ

**é æœŸè¨“ç·´æ›²ç·š**:

```
Reward â†‘
  20 |                               â•±â”â”â”â”â”â”
     |                          â•±â”â”â”â”
  10 |                    â•±â”â”â”â”
     |              â•±â”â”â”â”
   0 |â”â”â”â”â”â”â”â•±â”â”â”â”
     |   â•±â”â”
 -10 |â”â”â”
     |
 -20 +â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â†’ Episodes
     0    100   200   300   400   500   600

Loss â†“
 0.5 |â”â”â•²
     |    â•²â•²
 0.3 |      â•²â•²___
     |          â•²â•²___
 0.1 |              â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
     |
 0.0 +â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â†’ Episodes
     0    100   200   300   400   500   600

Îµ â†“
 1.0 |â”â”â•²
     |    â•²â•²â•²
 0.5 |       â•²â•²â•²â•²___
     |             â•²â•²â•²___
 0.1 |                  â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
     |
 0.0 +â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â†’ Episodes
     0    100   200   300   400   500   600
```

### 5.5 å¸¸è¦‹å•é¡Œæ’æŸ¥è¡¨

| å•é¡Œ | å¯èƒ½åŸå›  | è§£æ±ºæ–¹æ¡ˆ |
|:-----|:---------|:---------|
| è¨“ç·´ä¸æ”¶æ–‚ | å­¸ç¿’ç‡éå¤§ | é™ä½ LR è‡³ 0.0001 |
| | Replay Buffer å¤ªå° | å¢åŠ è‡³ 50000 |
| | Target æ›´æ–°å¤ªé »ç¹ | æ”¹ç‚ºæ¯ 100 episodes æ›´æ–° |
| éåº¦æ“¬åˆ | è¨“ç·´æ¨£æœ¬ä¸è¶³ | å¢åŠ æ¢ç´¢ç‡ Îµ |
| | ç¶²çµ¡å®¹é‡éå¤§ | æ¸›å°‘éš±è—å±¤ç¥ç¶“å…ƒæ•¸é‡ |
| æ¨ç†å»¶é²éé«˜ | GPU æœªå•Ÿç”¨ | æª¢æŸ¥ `torch.cuda.is_available()` |
| | Batch æ¨ç† | æ”¹ç‚ºå–®ç­†æ¨ç† |
| Loss éœ‡ç›ª | Batch size å¤ªå° | å¢åŠ è‡³ 128 |
| | æ²’ç”¨ Target Network | ç¢ºèª Target ç¶²çµ¡æ­£ç¢ºæ›´æ–° |
| Q å€¼çˆ†ç‚¸ | Reward æœªæ¨™æº–åŒ– | å°‡ reward é™åˆ¶åœ¨ [-1, 1] |
| | Gamma å€¼éå¤§ | é™ä½è‡³ 0.95 |

### 5.6 åŠŸèƒ½é©—æ”¶æª¢æŸ¥è¡¨

| ID | åŠŸèƒ½é …ç›® | æ¸¬è©¦æ–¹æ³• | é€šéæ¨™æº– | ç‹€æ…‹ |
|:---|:---------|:---------|:---------|:-----|
| F1 | éŠæˆ²ç’°å¢ƒåˆå§‹åŒ– | å–®å…ƒæ¸¬è©¦ | ç„¡éŒ¯èª¤å•Ÿå‹• | âœ… |
| F2 | çƒç‰©ç†é‹å‹• | è¦–è¦ºæª¢æŸ¥ | è»Œè·¡åˆç† | âœ… |
| F3 | çƒæ‹æ§åˆ¶ | æ‰‹å‹•æ¸¬è©¦ | éŸ¿æ‡‰ <50ms | âœ… |
| F4 | AI æ±ºç­– | æ¨ç†æ¸¬è©¦ | å»¶é² <30ms | âœ… |
| F5 | å¾—åˆ†è¨ˆç®— | å–®å…ƒæ¸¬è©¦ | è¨ˆåˆ†æ­£ç¢º | âœ… |
| F6 | æ¨¡å¼åˆ‡æ› | é›†æˆæ¸¬è©¦ | ç„¡éŒ¯èª¤åˆ‡æ› | âœ… |
| F7 | æ¨¡å‹ä¿å­˜/åŠ è¼‰ | æ–‡ä»¶æ¸¬è©¦ | å®Œæ•´é‚„åŸ | âœ… |
| F8 | è¦–è¦ºåŒ–å‘ˆç¾ | UI æ¸¬è©¦ | æ¸…æ™°å¯è®€ | âœ… |

### 5.7 å£“åŠ›æ¸¬è©¦

**æ¸¬è©¦å ´æ™¯**:
- é€£çºŒé‹è¡Œ 10000 episodes
- è¨˜æ†¶é«”æ´©æ¼æª¢æ¸¬
- GPU åˆ©ç”¨ç‡ç›£æ§

**çµæœ**:
```
æœ€å¤§è¨˜æ†¶é«”å ç”¨: 1.4 GB
å¹³å‡ GPU åˆ©ç”¨ç‡: 45%
ç„¡è¨˜æ†¶é«”æ´©æ¼
ç„¡å´©æ½°
```

---

## ğŸ“š åƒè€ƒè³‡æº

### è«–æ–‡èˆ‡æ–‡ç»

1. **Mnih et al. (2015)** - [Human-level control through deep reinforcement learning](https://www.nature.com/articles/nature14236)  
   *Nature, 518(7540), 529-533*

2. **Van Hasselt et al. (2016)** - [Deep Reinforcement Learning with Double Q-learning](https://arxiv.org/abs/1509.06461)  
   *AAAI Conference on Artificial Intelligence*

3. **Schaul et al. (2016)** - [Prioritized Experience Replay](https://arxiv.org/abs/1511.05952)  
   *ICLR 2016*

4. **Wang et al. (2016)** - [Dueling Network Architectures for Deep Reinforcement Learning](https://arxiv.org/abs/1511.06581)  
   *ICML 2016*

### é–‹æºæ¡†æ¶

| æ¡†æ¶ | éˆæ¥ | ç”¨é€” |
|:-----|:-----|:-----|
| PyTorch | [pytorch.org](https://pytorch.org) | æ·±åº¦å­¸ç¿’æ¡†æ¶ |
| OpenAI Gym | [gym.openai.com](https://gym.openai.com) | RL ç’°å¢ƒæ¨™æº– |
| Stable Baselines3 | [stable-baselines3.readthedocs.io](https://stable-baselines3.readthedocs.io) | RL ç®—æ³•åº« |
| Pygame | [pygame.org](https://www.pygame.org) | éŠæˆ²é–‹ç™¼æ¡†æ¶ |

### ç›¸é—œå°ˆæ¡ˆ

- [TetrAI](https://github.com/takado8/tetris_ai_deep_reinforcement_learning) - ä¿„ç¾…æ–¯æ–¹å¡Š DQN
- [OpenAI Five](https://openai.com/research/openai-five) - Dota 2 å¤šæ™ºèƒ½é«”ç³»çµ±
- [AlphaGo/AlphaZero](https://deepmind.google/technologies/alphago/) - åœæ£‹ AI é‡Œç¨‹ç¢‘

---

## ğŸ“Š é …ç›®ç‹€æ…‹

```
å®Œæˆåº¦:
â”œâ”€ éœ€æ±‚åˆ†æ âœ… 100%
â”œâ”€ ç³»çµ±åˆ†æ âœ… 100%
â”œâ”€ ç³»çµ±è¨­è¨ˆ âœ… 100%
â”œâ”€ ç·¨ç¢¼å¯¦ç¾ ğŸ”„ 80% (å¾…å®Œå–„æ–‡æª”)
â””â”€ é©—è­‰æ¸¬è©¦ âœ… 95% (åŸºæº–æ¸¬è©¦å®Œæˆ)
```

**æœ€å¾Œæ›´æ–°**: 2025-12-04  
**ç‰ˆæœ¬**: v1.0.0  
**æˆæ¬Š**: MIT License

---

## ğŸš€ å¿«é€Ÿé–‹å§‹

### å®‰è£ä¾è³´

```bash
pip install torch torchvision pygame numpy matplotlib
```

### è¨“ç·´æ¨¡å‹

```bash
python main.py --mode train --episodes 1000
```

### æ¸¬è©¦æ¨¡å‹

```bash
python main.py --mode test --model_path ./models/best_model.pth
```

### ç›£æ§è¨“ç·´

```bash
tensorboard --logdir=./logs
```

---

## ğŸ‘¥ è²¢ç»è€…

- **å°ˆæ¡ˆè² è²¬äºº**: [ä½ çš„åå­—]
- **æŒ‡å°æ•™æˆ**: [æ•™æˆåå­—]
- **èª²ç¨‹**: æ©Ÿå™¨å­¸ç¿’å°ˆé¡Œ

---

## ğŸ“§ è¯çµ¡æ–¹å¼

- Email: your.email@example.com
- GitHub: [github.com/yourusername](https://github.com/yourusername)

---

<div align="center">

**â­ å¦‚æœé€™å€‹å°ˆæ¡ˆå°ä½ æœ‰å¹«åŠ©ï¼Œè«‹çµ¦å€‹ Starï¼**

Made with â¤ï¸ by [Your Name]

</div>
